# Standard
from typing import Dict, Iterable, List, Optional, Tuple
import random

# Local
from fms_dgt.base.databuilder import GenerationDataBuilder
from fms_dgt.base.prompt import JinjaPromptTemplate
from fms_dgt.base.registry import register_data_builder
from fms_dgt.base.task import GenerationTask
from fms_dgt.core.blocks.llm import LMProvider
from fms_dgt.core.blocks.validators.lm_judge import LMJudgeValidator
from fms_dgt.public.blocks.magpie.tag import MagpieTagger
from fms_dgt.public.databuilders.instructlab.skills.data_objects import SkillsData
from fms_dgt.public.databuilders.instructlab.skills.task import (
    SkillsTask,
)
from fms_dgt.utils import dgt_logger
import fms_dgt.public.databuilders.instructlab.skills.utils as utils


@register_data_builder("instructlab/skills")
class SkillsDataBuilder(GenerationDataBuilder):
    """Class for generating instruction-response pairs using seed data from the `compositional_skills` branch of InstructLab's taxonomy"""

    TASK_TYPE: GenerationTask = SkillsTask

    # generator is the main generator that will produce the synthetic examples
    generator: LMProvider

    # tagger is the Magpie based tagger to rate synthetic examples
    tagger: Optional[MagpieTagger]

    # validator is the validator which checks a generated api call is correct w.r.t. the input specification
    validator: LMJudgeValidator

    def call_with_task_list(
        self, tasks: List[SkillsTask], request_idx: int
    ) -> Iterable[SkillsData]:
        """Executes data builder __call__ function for all in-progress tasks. Is executed in the inner loop of `execute_tasks`

        Args:
            tasks (List[SkillsTask]): List of in-progress tasks
            request_idx (int): The iteration of `execute_tasks` this method was called at

        Returns:
            Iterable[SkillsData]: List of data instances generated by the __call__ function
        """
        outputs = []
        for task in tasks:
            dgt_logger.info("=" * 99)
            dgt_logger.info('\t\tTask: "%s"', task.name)
            dgt_logger.info("=" * 99)

            outputs.extend(
                self(
                    seed_data=task.get_batch_examples(),
                    prompt_templates=task.prompt_templates,
                    num_icl_examples_per_prompt=task.num_icl_examples_per_prompt,
                    num_questions_to_generate_per_prompt=task.num_questions_to_generate_per_prompt,
                )
            )

            dgt_logger.info("=" * 99)

        return outputs

    def __call__(
        self,
        seed_data: List[SkillsData],
        prompt_templates: Dict[str, JinjaPromptTemplate],
        num_icl_examples_per_prompt: int = 3,
        num_questions_to_generate_per_prompt: int = 5,
    ) -> List[SkillsData]:
        # Initialize necessary variables
        generated_data_points: List[SkillsData] = []
        freeform_question_answer_pairs_seed_data = []
        context_based_question_answer_pairs_seed_data = []
        for data_point in seed_data:
            if data_point.context:
                context_based_question_answer_pairs_seed_data.append(data_point)
            else:
                freeform_question_answer_pairs_seed_data.append(data_point)

        # Generate context based and freeform question answer pairs appropriately
        if context_based_question_answer_pairs_seed_data:
            context_based_question_answer_pairs = (
                self._generate_context_based_question_answer_pairs(
                    data_points=context_based_question_answer_pairs_seed_data,
                    prompt_templates=prompt_templates,
                    num_questions_to_generate_per_prompt=num_questions_to_generate_per_prompt,
                )
            )

            if hasattr(self, "tagger"):
                # Invoke tagger
                tagged_data_points = self.tagger(
                    context_based_question_answer_pairs, disable_tqdm=True
                )

                # Add to final generated data
                generated_data_points.extend(tagged_data_points)
            else:
                generated_data_points.extend(context_based_question_answer_pairs)

        # If context is not present, we pass the data to `_generate_freeform_question_answer_pairs`
        else:
            qa_pairs_without_context = self._generate_freeform_question_answer_pairs(
                seed_data=freeform_question_answer_pairs_seed_data,
                prompt_templates=prompt_templates,
                num_icl_examples_per_prompt=num_icl_examples_per_prompt,
                num_questions_to_generate_per_prompt=num_questions_to_generate_per_prompt,
            )

            if hasattr(self, "tagger"):
                # Invoke tagger
                tagged_data_points = self.tagger(qa_pairs_without_context, disable_tqdm=True)

                # Add to final generated data
                generated_data_points.extend(tagged_data_points)
            else:
                generated_data_points.extend(qa_pairs_without_context)

        return generated_data_points

    # ===========================================================================
    #                       CONTEXT BASED QUETION-ANSWERS
    # ===========================================================================
    def _generate_context_based_question_answer_pairs(
        self,
        data_points: List[SkillsData],
        prompt_templates: Dict[str, JinjaPromptTemplate],
        num_questions_to_generate_per_prompt: int = 5,
    ) -> Tuple[List[SkillsData], int]:
        shuffled_data = data_points + []
        random.shuffle(shuffled_data)

        # Generate contexts
        dgt_logger.info("Generating contexts..")
        generated_contexts = self._generate_contexts(
            shuffled_data, prompt_template=prompt_templates["context_generation"]
        )

        # Generate questions based on the generated context
        dgt_logger.info("Generating context based questions..")
        context_based_questions = self._generate_context_based_questions(
            data_points=generated_contexts,
            seed_data=data_points,
            prompt_template=prompt_templates["context_based_question_generation"],
            num_questions_to_generate_per_prompt=num_questions_to_generate_per_prompt,
        )

        # Validate generated questions
        dgt_logger.info("Validating questions..")
        valid_questions = self._validate_context_based_questions(
            context_based_questions,
            prompt_template=prompt_templates["context_based_question_validation"],
        )

        # Generate responses
        dgt_logger.info("Generating answers..")
        context_based_question_answer_pairs = self._generate_responses_for_context_based_questions(
            data_points=valid_questions,
            seed_data=data_points,
            prompt_template=prompt_templates["context_based_answer_generation"],
        )

        # Step 5: Validate final QA pairs
        dgt_logger.info("Validating context based question-answer pairs..")
        validated_context_based_question_answer_pairs = (
            self._validate_responses_for_context_based_questions(
                data_points=context_based_question_answer_pairs,
                prompt_template=prompt_templates["context_based_answer_validation"],
            )
        )

        # Return
        return validated_context_based_question_answer_pairs

    def _generate_contexts(self, seed_data: List[SkillsData], prompt_template: JinjaPromptTemplate):
        # Build generator inputs
        generator_inputs: List[Dict] = [
            {
                "input": prompt_template.encode(
                    render_dict={
                        "task_description": data_point.task_description,
                        "context": data_point.context,
                    }
                ),
                "gen_kwargs": {"stop": prompt_template.stop},
                "reference": data_point,
            }
            for data_point in seed_data
        ]

        # Invoke generator
        generator_outputs = self.generator(generator_inputs)

        # Process generated outputs
        outputs = []
        for generator_output in generator_outputs:
            outputs.append(
                SkillsData(
                    task_name=generator_output["reference"].task_name,
                    is_seed=False,
                    task_description=generator_output["reference"].task_description,
                    instruction=None,
                    response=None,
                    context=utils.post_process_context(generator_output["result"]),
                )
            )

        # Return
        return outputs

    def _generate_context_based_questions(
        self,
        data_points: List[SkillsData],
        seed_data: List[SkillsData],
        prompt_template: JinjaPromptTemplate,
        num_questions_to_generate_per_prompt: int,
    ) -> List[SkillsData]:
        # Build generator inputs
        generator_inputs: List[Dict] = []
        for data_point in data_points:
            # Randomly select in-context learning (ICL) example
            icl_example = random.choice(seed_data)

            # Build generator input
            generator_inputs.append(
                {
                    "input": prompt_template.encode(
                        render_dict={
                            "num_samples": num_questions_to_generate_per_prompt,
                            "task_description": data_point.task_description,
                            "icl_context": icl_example.context,
                            "icl_question": icl_example.instruction,
                            "context": data_point.context,
                        }
                    ),
                    "gen_kwargs": {"stop": prompt_template.stop},
                    "reference": data_point,
                }
            )

        # Invoke generator
        generator_outputs = self.generator(generator_inputs)

        # Process outputs
        outputs = []
        for generator_output in generator_outputs:
            questions = utils.post_process_questions(generator_output["result"])
            for question in questions:
                outputs.append(
                    SkillsData(
                        task_name=generator_output["reference"].task_name,
                        is_seed=False,
                        task_description=generator_output["reference"].task_description,
                        instruction=question,
                        response=None,
                        context=generator_output["reference"].context,
                    )
                )

        # Return
        return outputs

    def _validate_context_based_questions(
        self, data_points: List[SkillsData], prompt_template: JinjaPromptTemplate
    ):
        # Build validator inputs
        validator_inputs: List[Dict] = [
            {
                "input": prompt_template.encode(
                    render_dict={
                        "task_description": data_point.task_description,
                        "question": data_point.instruction,
                        "context": data_point.context,
                    }
                ),
                "success_func": lambda x: utils.parse_response_string(x) == 1.0,
                "gen_kwargs": {"stop": prompt_template.stop},
                "reference": data_point,
                "store_names": self.get_block_store_names(
                    block_name=self.validator.name, task_name=data_point.task_name
                ),
            }
            for data_point in data_points
        ]

        # Invoke validator
        validator_outputs = self.validator(validator_inputs)

        # Return valid data points
        return [validator_output["reference"] for validator_output in validator_outputs]

    def _generate_responses_for_context_based_questions(
        self,
        data_points: List[SkillsData],
        seed_data: List[SkillsData],
        prompt_template: JinjaPromptTemplate,
    ):
        # Build generator inputs
        generator_inputs: List[Dict] = []
        for data_point in data_points:
            # Randomly select in-context learning (ICL) example
            icl_example = random.choice(seed_data)

            # Build generator input
            generator_inputs.append(
                {
                    "input": prompt_template.encode(
                        render_dict={
                            "icl_context": icl_example.context,
                            "icl_question": icl_example.instruction,
                            "icl_response": icl_example.response,
                            "context": data_point.context,
                            "question": data_point.instruction,
                        }
                    ),
                    "gen_kwargs": {"stop": prompt_template.stop},
                    "reference": data_point,
                }
            )

        # Invoke generator
        generator_outputs = self.generator(generator_inputs)

        # Process outputs
        outputs = []
        for generator_output in generator_outputs:
            data_point = generator_output["reference"]
            data_point.response = generator_output["result"]
            outputs.append(data_point)

        # Return
        return outputs

    def _validate_responses_for_context_based_questions(
        self, data_points: List[SkillsData], prompt_template: JinjaPromptTemplate
    ):
        # Build validator inputs
        validator_inputs: List[Dict] = [
            {
                "input": prompt_template.encode(render_dict={}),
                "success_func": lambda x: utils.parse_response_string(x) > 1.0,
                "gen_kwargs": {"stop": prompt_template.stop},
                "reference": data_point,
                "store_names": self.get_block_store_names(
                    block_name=self.validator.name, task_name=data_point.task_name
                ),
            }
            for data_point in data_points
        ]

        # Invoke validator
        validator_outputs = self.validator(validator_inputs)

        # Return valid context based question answer pairs
        return [validator_output["reference"] for validator_output in validator_outputs]

    # ===========================================================================
    #                       FREEFORM QUESTION-ANSWERS
    # ===========================================================================
    def _generate_freeform_question_answer_pairs(
        self,
        seed_data: List[SkillsData],
        prompt_templates: Dict[str, JinjaPromptTemplate],
        num_icl_examples_per_prompt: int = 3,
        num_questions_to_generate_per_prompt: int = 5,
    ) -> Tuple[List[SkillsData], int]:
        shuffled_data = seed_data + []
        random.shuffle(shuffled_data)

        # Generate freeform questions
        dgt_logger.info("Generating freefrom questions..")
        generated_freeform_questions = self._generate_freeform_questions(
            seed_data=shuffled_data,
            prompt_template=prompt_templates["freeform_question_generation"],
            num_icl_examples_per_prompt=num_icl_examples_per_prompt,
            num_questions_to_generate_per_prompt=num_questions_to_generate_per_prompt,
        )

        # Validate generated freeform questions
        dgt_logger.info("Validating questions..")
        validated_questions = self._validate_freeform_questions(
            data_points=generated_freeform_questions,
            prompt_template=prompt_templates["freeform_question_validation"],
        )

        # Generate responses for valid freeform questions
        dgt_logger.info("Generating answers..")
        freeform_questions_with_responses = self._generate_responses_to_freeform_questions(
            data_points=validated_questions,
            seed_data=seed_data,
            prompt_template=prompt_templates["answer_generation"],
        )

        # Validate final QA pairs
        dgt_logger.info("Validating final QA pairs..")
        valid_freeform_question_answers = self._validate_responses_to_freeform_questions(
            data_points=freeform_questions_with_responses,
            prompt_template=prompt_templates["answer_validation"],
        )

        # Return
        return valid_freeform_question_answers

    def _generate_freeform_questions(
        self,
        seed_data: List[SkillsData],
        prompt_template: JinjaPromptTemplate,
        num_icl_examples_per_prompt: int = 3,
        num_questions_to_generate_per_prompt: int = 5,
    ) -> List[SkillsData]:
        # Build generator inputs
        generator_inputs: List[Dict] = []
        for i in range(0, len(seed_data), num_icl_examples_per_prompt):
            # Fetch in-context learning (ICL) examples
            icl_examples = seed_data[i : i + num_icl_examples_per_prompt]

            # Build generator input
            generator_inputs.append(
                {
                    "input": prompt_template.encode(
                        render_dict={
                            "num_samples": num_questions_to_generate_per_prompt,
                            "task_description": seed_data[0].task_description,
                            "icl_question": "\n".join(
                                [
                                    f"### Question {example_index}: {example.instruction}"
                                    for example_index, example in enumerate(icl_examples, start=1)
                                ]
                            ),
                        }
                    ),
                    "gen_kwargs": {"stop": prompt_template.stop},
                    "references": icl_examples,
                }
            )

        # Invoke generator
        generator_outputs = self.generator(generator_inputs)

        # Process outputs
        outputs = []
        for generator_output in generator_outputs:
            questions = utils.post_process_questions(generator_output["result"])
            used_icl_examples = generator_output["references"]
            for question in questions:
                outputs.append(
                    SkillsData(
                        task_name=used_icl_examples[0].task_name,
                        is_seed=False,
                        task_description=used_icl_examples[0].task_description,
                        instruction=question,
                        response=None,
                    )
                )

        return outputs

    def _validate_freeform_questions(
        self,
        data_points: List[SkillsData],
        prompt_template: JinjaPromptTemplate,
    ):
        # Build validator inputs
        validator_inputs: List[Dict] = [
            {
                "input": prompt_template.encode(
                    render_dict={
                        "task_description": data_point.task_description,
                        "question": data_point.instruction,
                    }
                ),
                "success_func": lambda x: utils.parse_response_string(x) == 1.0,
                "gen_kwargs": {"stop": prompt_template.stop},
                "reference": data_point,
                "store_names": self.get_block_store_names(
                    block_name=self.validator.name, task_name=data_point.task_name
                ),
            }
            for data_point in data_points
        ]

        # Invoke validator
        validator_outputs = self.validator(validator_inputs)

        # Return valid freeform questions
        return [entry["reference"] for entry in validator_outputs]

    def _generate_responses_to_freeform_questions(
        self,
        data_points: List[SkillsData],
        seed_data: List[SkillsData],
        prompt_template: JinjaPromptTemplate,
    ):
        generator_inputs: List[Dict] = []
        for data_point in data_points:
            # Randomly select in-context learning (ICL) example
            icl_example = random.choice(seed_data)

            # Build generator input
            generator_inputs.append(
                {
                    "input": prompt_template.encode(
                        render_dict={
                            "icl_question": icl_example.instruction,
                            "icl_response": icl_example.response,
                            "question": data_point.instruction,
                        }
                    ),
                    "gen_kwargs": {"stop": prompt_template.stop},
                    "reference": data_point,
                }
            )

        # Invoke generator
        generator_outputs = self.generator(generator_inputs)

        # Process outputs
        outputs = []
        for generator_output in generator_outputs:
            output = generator_output["reference"]
            output.response = generator_output["result"]
            outputs.append(output)

        # Return
        return outputs

    def _validate_responses_to_freeform_questions(
        self, data_points: List[SkillsData], prompt_template: JinjaPromptTemplate
    ):
        # Build validator inputs
        validator_inputs: List[Dict] = [
            {
                "input": prompt_template.encode(
                    render_dict={
                        "question": data_point.instruction,
                        "answer": data_point.response,
                    }
                ),
                "success_func": lambda x: utils.parse_response_string(x) > 1.0,
                "gen_kwargs": {"stop": prompt_template.stop},
                "reference": data_point,
                "store_names": self.get_block_store_names(
                    block_name=self.validator.name, task_name=data_point.task_name
                ),
            }
            for data_point in data_points
        ]

        # Invoke validator
        validator_outputs = self.validator(validator_inputs)

        # Return valid freeform question answer pairs
        return [validator_output["reference"] for validator_output in validator_outputs]
