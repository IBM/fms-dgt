# Standard
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Mapping, Optional, Union
import os
import random

# Third Party
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Local
from fms_dgt.base.data_objects import DataBuilderConfig
from fms_dgt.base.databuilder import TransformationDataBuilder
from fms_dgt.base.registry import get_block, register_data_builder
from fms_dgt.base.task import TransformationTask
from fms_dgt.constants import TYPE_KEY
from fms_dgt.core.blocks.llm import MODEL_ID_OR_PATH, LMProvider
from fms_dgt.public.databuilders.time_series.task import (
    TimeSeriesInputData,
    TimeSeriesOutputData,
    TimeSeriesTask,
)
from fms_dgt.public.databuilders.time_series.trainer import SDForgerTuningBlock
from fms_dgt.public.databuilders.time_series.utils import (
    convert_texts_to_tabular_data,
    fica_embed_data,
    fica_transform_to_original_feature_space,
    fpc_embed_data,
    fpc_transform_to_original_feature_space,
    get_feature_distribution,
    preprocess_train_data,
)
from fms_dgt.utils import dgt_logger, init_dataclass_from_dict


# ===========================================================================
#                       DATA OBJECTS
# ===========================================================================
@dataclass(kw_only=True)
class TimeSeriesBuilderConfig(DataBuilderConfig):
    """Class for time-series data builder"""

    target: Dict = None  # Holds the LLM config for finetuning


# ===========================================================================
#                       MAIN CLASS
# ===========================================================================
@register_data_builder("time_series")
class TimeSeriesDataBuilder(TransformationDataBuilder):
    """Timeseries data builder"""

    TASK_TYPE: TransformationTask = TimeSeriesTask

    # trainer is the trainer block used for fine-tuning
    trainer: SDForgerTuningBlock

    def __init__(
        self,
        *args: Any,
        config: Union[Mapping, TimeSeriesBuilderConfig] = None,
        **kwargs: Any,
    ):
        config: TimeSeriesBuilderConfig = init_dataclass_from_dict(config, TimeSeriesBuilderConfig)

        super().__init__(*args, config=config, **kwargs)

    def call_with_task_list(self, tasks: List[TimeSeriesTask]) -> Iterable[TimeSeriesOutputData]:
        """Executes data builder __call__ function for all in-progress tasks. Is executed in the inner loop of `execute_tasks`

        Args:
            tasks (List[TimeSeriesSdgTask]): List of in-progress tasks

        Returns:
            Iterable[TimeSeriesOutputSdgData]: List of data instances generated by the __call__ function
        """
        outputs = []
        for task in tasks:
            dgt_logger.info("=" * 99)
            dgt_logger.info('\t\tTask: "%s"', task.name)
            dgt_logger.info("=" * 99)

            outputs.extend(
                self(
                    seed_data=task.get_batch_examples(),
                    data_params=task.data_params,
                    sdforger_params=task.sdforger_params,
                    output_dir=task.runner_config.output_dir,
                )
            )

            dgt_logger.info("=" * 99)

        return outputs

    def __call__(
        self,
        seed_data: List[TimeSeriesInputData],
        data_params: Dict,
        sdforger_params: Dict,
        output_dir: str,
    ) -> List[TimeSeriesOutputData]:

        task_name, task_description = seed_data[0].task_name, seed_data[0].task_description

        # Target LLM args
        target_llm_args = self.config.target

        # Create pandas dataframe with seed data
        train_data = pd.DataFrame([data.observations for data in seed_data])

        # Prepare training params
        train_params = sdforger_params | self.trainer.training_args

        # Preprocess training data
        dgt_logger.info("-" * 64)
        dgt_logger.info("\t\tPREPROCESS CHANNELS")
        dgt_logger.info("-" * 64)

        preprocessed_data, original_data, list_fitted_scaler = preprocess_train_data(
            train_data,
            **data_params,
            min_windows_length=train_params["min_windows_length"],
            min_windows_number=train_params["min_windows_number"],
        )

        # Transform time-series data to structured tabular data
        dgt_logger.info("-" * 64)
        dgt_logger.info("\t\tFROM PREPROCESSED DATA TO TABULAR DATA")
        dgt_logger.info("-" * 64)

        if train_params["embedding_type"] in ("fpc", "fpc-filled"):
            (
                embedded_data,
                embedding_dims,
                data_embedded,
                data_embedded_full,
                fpc_basis,
                fpc_basis_full,
            ) = fpc_embed_data(
                preprocessed_data,
                train_params["embedding_dim"],
                train_params["variance_explained"],
            )
        elif train_params["embedding_type"] in ("fica"):
            (
                embedded_data,
                embedding_dims,
                data_embedded,
                fica_mixing,
                fica_mean,
            ) = fica_embed_data(
                preprocessed_data,
                train_params["embedding_dim"],
                train_params["variance_explained"],
            )
        else:
            raise ValueError(
                f"embedding_type {train_params['embedding_type']} unknown. Please use one of ['fpc', 'fpc-filled', 'fica']."
            )

        dgt_logger.debug("\n%s", embedded_data)

        # Finetune LLM with embedded data
        dgt_logger.info("-" * 64)
        dgt_logger.info("\t\tFINETUNING LLM")
        dgt_logger.info("-" * 64)

        # But first, record original training dataset and its properties
        original_dataset = embedded_data.copy(deep=True)

        # Finetune model
        tuned_model_id_or_path = self.trainer(
            output_dir=os.path.join(output_dir, task_name),
            dataset=embedded_data,
            model_args=target_llm_args,
            sdforger_params=sdforger_params,
        )

        # Generate new embedding rows using the fine-tuned LLM
        dgt_logger.info("-" * 64)
        dgt_logger.info("\t\tGENERATING DATA USING FINE TUNED LLM")
        dgt_logger.info("-" * 64)

        # Initialize target LLM generator using the tuned model path
        target_llm_args[MODEL_ID_OR_PATH] = tuned_model_id_or_path
        tuned_llm: LMProvider = get_block(
            target_llm_args[TYPE_KEY],
            **target_llm_args,
        )

        # Generate synthetic data points using the llm
        generated_data = self.generate_data(
            llm=tuned_llm,
            sdforger_params=sdforger_params,
            embedded_dims=embedding_dims,
            original_dataset=original_dataset,
        )

        # Transform data back to the original feature space
        if train_params["embedding_type"] in ("fpc", "fpc-filled"):
            generated_data = fpc_transform_to_original_feature_space(
                generated_data,
                preprocessed_data,
                train_params["embedding_type"],
                embedding_dims,
                data_embedded_full,
                data_embedded,
                fpc_basis,
                fpc_basis_full,
            )
        elif train_params["embedding_type"] in ("fica"):
            generated_data = fica_transform_to_original_feature_space(
                generated_data, preprocessed_data, data_embedded, fica_mixing, fica_mean
            )

        self.plot_visualization(
            original_data=original_data,
            generated_data=generated_data,
            scalers=list_fitted_scaler,
            channel_names=data_params["train_channels"],
            output_path=os.path.join(output_dir, task_name, "plot_generated_data.pdf"),
        )

        return [
            TimeSeriesOutputData(
                task_name=task_name,
                task_description=task_description,
                generated_time_series={
                    data_params["train_channels"][var]: generated_data[var, sample, :].tolist()
                    for var in range(generated_data.shape[0])
                },
            )
            for sample in range(generated_data.shape[1])
        ]

    def finetune(self) -> str:
        pass

    def generate_data(
        self,
        llm: LMProvider,
        sdforger_params: Dict,
        embedded_dims: List[int],
        original_dataset: pd.DataFrame,
        check_distribution: bool = True,
    ) -> np.array:
        """Generate data using the llm instance.

        Args:
            llm (LMProvider): The large language model instance
            sdforger_params (Dict): Dictionary of SDForger params.
            embedded_dims (list): Embedding dimension size.
            original_dataset (pd.DataFrame): Original dataset used for finetuning
            check_distribution (bool, optional): Defaults to True.

        Returns:
            np.array: generated time-series in numpy array.
        """
        min_outputs_to_generate = sdforger_params["min_outputs_to_generate"]
        max_outputs_to_generate = sdforger_params["max_outputs_to_generate"]
        generation_batch_size = sdforger_params["inference_batch"]
        decimal = sdforger_params["input_tokens_precision"]

        # Init list for generated DataFrames
        dfs = []

        total_samples_generated = 0
        stop_criterion_satisfied = False

        # Initialize list to monitor data generation
        feature_distribution = {
            col: get_feature_distribution(original_dataset[col].tolist())
            for col in original_dataset.columns
        }
        dataset_columns = original_dataset.columns.to_list()
        numerical_columns = [
            col for col in dataset_columns if feature_distribution[col][0] == "numerical"
        ]
        split_indices = np.cumsum(embedded_dims)[:-1]  # Compute split points based on embedded_dims
        original_data_splits = np.split(
            original_dataset[numerical_columns].values, split_indices, axis=1
        )
        old_l2_norms_splits = [
            list(np.linalg.norm(split.astype(float), axis=1)) for split in original_data_splits
        ]

        norms_diversity = [[] for _ in embedded_dims]

        def init_tokens(
            tokenizer,
            permute: bool = True,
            start_col_dist: bool = False,
            distribution: Optional[str] = None,
            init_value: bool = False,
            text_template: str = "fim_template",
        ):
            if text_template == "base_template":
                if not distribution:

                    def distrib_nor_f(c):
                        np.random.normal(
                            feature_distribution[c][2]["mean"],
                            feature_distribution[c][2]["std"],
                            size=1,
                        ).item()

                    def distrib_r_c_f(c):
                        np.random.choice(feature_distribution[c][2]["data"]) + np.random.normal(
                            0, 0.1 * feature_distribution[c][2]["std"]
                        )

                if not start_col_dist:
                    if permute:
                        col_starter = random.choices(dataset_columns, k=generation_batch_size)
                    else:
                        col_starter = [dataset_columns[0]] * generation_batch_size
                    if init_value:
                        starter = [
                            (
                                (c, f"{distrib_nor_f(c):.{decimal}f}")
                                if feature_distribution[c][1]
                                else (c, f"{distrib_r_c_f(c):.{decimal}f}")
                            )
                            for c in col_starter
                        ]
                    else:
                        starter = [(c, "") for c in col_starter]

                    input_prompts = [
                        "".join([c, " is ", v, "," if init_value else ""]) for c, v in starter
                    ]
                    input_prompts = [
                        s[:-1] if s.endswith(" ") else s for s in input_prompts
                    ]  # remove last space of the prompt if present

                    len_input = [len(tokenizer(prompt)["input_ids"]) for prompt in input_prompts]
                    max_tokens = max(len_input)
                    padding = tokenizer.pad_token
                    input_prompts = [
                        (padding * (max_tokens - len) + prompt if len < max_tokens else prompt)
                        for len, prompt in zip(len_input, input_prompts)
                    ]

                else:
                    raise NotImplementedError

            elif text_template == "fim_template":
                col_starter = []
                if permute:
                    col_list = dataset_columns.copy()
                    for _ in range(0, generation_batch_size):
                        random.shuffle(col_list)
                        col_starter.append(col_list.copy())

                else:
                    col_starter = [dataset_columns.copy()] * generation_batch_size
                input_prompts = [
                    "Input: " + " is [blank], ".join(starter) + " is [blank] [sep] Target:"
                    for starter in col_starter
                ]
                len_input = [len(tokenizer(prompt)["input_ids"]) for prompt in input_prompts]
                max_tokens = max(len_input)
                padding = tokenizer.pad_token
                input_prompts = [
                    padding * (max_tokens - len) + prompt if len < max_tokens else prompt
                    for len, prompt in zip(len_input, input_prompts)
                ]

            elif text_template == "fim_template_textual_encoding":
                input_prompts = []

                for _ in range(generation_batch_size):

                    if permute:
                        col_list = dataset_columns.copy()
                        random.shuffle(col_list)

                    text_categorical = "".join(
                        [
                            (
                                f"{c} is {random.choice(list(feature_distribution[c][2].keys()))}, "
                                if feature_distribution[c][0] == "categorical"
                                else ""
                            )
                            for c in dataset_columns
                        ]
                    ).strip(", ")

                    text_input = "".join(
                        [
                            (
                                f"{c} is [blank], "
                                if feature_distribution[c][0] == "numerical"
                                else ""
                            )
                            for c in col_list
                        ]
                    ).strip(", ")

                    text = (
                        "Condition: "
                        + text_categorical
                        + " [sep] Input: "
                        + text_input
                        + " [sep] Target:"
                    )

                    input_prompts.append(text)

                len_input = [len(tokenizer(prompt)["input_ids"]) for prompt in input_prompts]
                max_tokens = max(len_input)
                padding = tokenizer.pad_token
                input_prompts = [
                    padding * (max_tokens - len) + prompt if len < max_tokens else prompt
                    for len, prompt in zip(len_input, input_prompts)
                ]

            return input_prompts

        try:
            attempt = 0

            while (
                total_samples_generated < min_outputs_to_generate
                or total_samples_generated < max_outputs_to_generate
            ):
                llm_outputs = llm(
                    [
                        {
                            "input": prompt,
                            "gen_kwargs": {
                                "repetition_penalty": 1.0,
                            },
                        }
                        for prompt in init_tokens(
                            tokenizer=llm.tokenizer,
                        )
                    ]
                )

                # Clean text
                textual_data = [
                    (output["input"] + output["result"]).replace("\n", " ").replace("\r", "")
                    for output in llm_outputs
                ]

                # Convert textual data to dataframe
                generated_df = convert_texts_to_tabular_data(
                    text=textual_data,
                    original_dataset=original_dataset,
                )

                generated_samples_count = generated_df.shape[0]
                dgt_logger.info("No. of samples generated: %d", generated_samples_count)

                # Remove rows where we have not generated anything
                generated_df = generated_df[~(generated_df == "NaN").any(axis=1)]
                generated_df = generated_df[~generated_df.isnull().any(axis=1)]

                # Remove rows where all values are NaN
                generated_df = generated_df.dropna(how="all")

                dgt_logger.info(
                    "No. of samples after dropping NaN: %d -> %d",
                    generated_samples_count,
                    generated_df.shape[0],
                )

                for col, dist in feature_distribution.items():
                    if dist[0] != "categorical":
                        coerced_series = pd.to_numeric(generated_df[col], errors="coerce")
                        generated_df = generated_df[
                            coerced_series.notnull() | generated_df[col].isna()
                        ]
                        generated_df[col] = generated_df[col].astype(float)

                # Append df_gen to dfs, then concatenate and remove duplicates across all generated samples
                dfs.append(generated_df)
                data = pd.concat(dfs).round(3).drop_duplicates()
                dedup_samples_count = data[total_samples_generated:].shape[0]

                dgt_logger.info(
                    "No. of samples after dropping duplicates: %d -> %d",
                    generated_df.shape[0],
                    dedup_samples_count,
                )

                if check_distribution:
                    # Identify numerical columns and partition new vs. old generated data
                    new_data = data[total_samples_generated:]  # New batch of generated samples

                    # Calculate multiple L2 norms for each subset based on embedded_dims
                    new_data_splits = np.split(
                        new_data[numerical_columns].values, split_indices, axis=1
                    )  # Split the data

                    # Calculate L2 norms for each subset
                    new_generated_norms = [
                        np.linalg.norm(split, axis=1) for split in new_data_splits
                    ]

                    # Calculate IQR bounds for each set of norms
                    bounds = []
                    for norms in old_l2_norms_splits:
                        Q1, Q3 = np.percentile(norms, [25, 75])
                        IQR = Q3 - Q1
                        bounds.append(
                            (Q1 - 3 * IQR, Q3 + 3 * IQR)
                        )  # Store lower and upper bounds for each subset

                    # Check if each row in the new data passes the IQR bounds for each subset norm
                    accepted_mask = np.ones(len(new_data), dtype=bool)
                    for i, (norms, (lower, upper)) in enumerate(zip(new_generated_norms, bounds)):
                        discard_mask = (norms < lower) | (
                            norms > upper
                        )  # Outliers for the current subset
                        accepted_mask &= (
                            ~discard_mask
                        )  # Update the accepted mask based on current subset check

                        # Print details for discarded norms, if any
                        if np.any(discard_mask):
                            dgt_logger.info(
                                "Var %d Discarded Norms: %s", i + 1, norms[discard_mask]
                            )

                    # Keep only accepted data within bounds across all subsets
                    data = pd.concat(
                        [data[:total_samples_generated], new_data[accepted_mask]]
                    )  # Update data with accepted norms

                    # Update old norms for each subset
                    for i, norms in enumerate(new_generated_norms):
                        old_l2_norms_splits[i].extend(norms[accepted_mask])

                if check_distribution:
                    dgt_logger.info(
                        "No. of samples after norm-check drop: %d -> %d",
                        dedup_samples_count,
                        sum(accepted_mask),
                    )

                # update lists
                delta = data.shape[0] - total_samples_generated

                dgt_logger.info(
                    "Samples generated (%d) in attempt %d. [Total samples generated: %d]",
                    delta,
                    attempt + 1,
                    data.shape[0],
                )

                for i, subset_norms in enumerate(old_l2_norms_splits):
                    rounding_factor = 3

                    # Round norms based on calculated rounding factor and compute diversity for the subset
                    unique_norms = len(set(np.round(subset_norms, rounding_factor)))
                    diversity_score = unique_norms / len(subset_norms)
                    norms_diversity[i].append(diversity_score)  # Proportion of unique rounded norms

                dgt_logger.info("Norms diversity: %s", norms_diversity)

                # important, update already generated
                total_samples_generated = data.shape[0]

                # Reset dfs to hold the combined unique samples for the next cycle
                dfs = [data]

                # check if we are generating valid instances
                attempt = (
                    attempt + 1 if delta < 1 else 0
                )  # Increment if no new generations, reset otherwise
                if attempt > 9:
                    dgt_logger.info(
                        "No new samples have been generated in the last 10 iterations. We halt the generation process"
                    )
                    break

                # check stopping criterion
                max_element = max(sublist[-1] for sublist in norms_diversity)
                if max_element < sdforger_params["norms_diversity_threshold"]:
                    stop_criterion_satisfied = True
                    if total_samples_generated > min_outputs_to_generate:
                        dgt_logger.info(
                            "The generation process has been halted due to the stopping criterion. - norms_diversity_threshold: %s",
                            sdforger_params["norms_diversity_threshold"],
                        )
                        break
                    else:
                        dgt_logger.info(
                            "The stopping criterion - {sdforger_params['norms_diversity_threshold']} - has been met but generation process is continuing as not enough samples generated yet!"
                        )

        except Exception as e:
            dgt_logger.error("Error occurred during token generation: %s", str(e))

        if not stop_criterion_satisfied:
            dgt_logger.info(
                "Stopping criterion not satisfied. Reached max generations. Samples generated: %d vs Samples requested: %d",
                total_samples_generated,
                max_outputs_to_generate,
            )

        return pd.concat(dfs).reset_index(drop=True).to_numpy()

    def plot_visualization(
        self,
        original_data: List[np.ndarray],
        generated_data: List[np.ndarray],
        scalers: List[StandardScaler],
        channel_names: List[str],
        output_path,
    ) -> None:
        # Inverse transform generated data
        rescaled_generated_data = []
        for var in range(len(original_data)):
            # Retrieve fitted standarscaler
            scaler = scalers[var]
            # Retrieve the generated data back to the original feature space
            rescaled_generated_data.append(scaler.inverse_transform(generated_data[var]))

        # Stack data for plotting
        rescaled_generated_data = np.stack(rescaled_generated_data, axis=0)
        original_data = np.stack(original_data, axis=0)

        # Setup plot layout
        num_vars = len(rescaled_generated_data)
        n_cols = 1  # Adjust based on how many plots per row you want
        n_rows = int(np.ceil(num_vars / n_cols))
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15 * n_cols, 5 * n_rows), squeeze=False)
        axes = axes.flatten()  # Flatten in case of single row

        # Plot each variable
        for var in range(num_vars):
            ax = axes[var]
            ax.plot(
                original_data[var][:50, 0:].T,
                "--",
                color="mediumturquoise",
                alpha=0.8,
                label="Original Data",
            )
            ax.plot(
                rescaled_generated_data[var][:25, 0:].T,
                color="orchid",
                lw=1,
                alpha=0.8,
                label="Synthetic Data",
            )
            ax.set_title(channel_names[var])

            handles, labels = ax.get_legend_handles_labels()
            unique = dict(zip(labels, handles))
            ax.legend(unique.values(), unique.keys(), loc="upper right")

        # Remove unused subplots
        for i in range(num_vars, len(axes)):
            fig.delaxes(axes[i])

        plt.tight_layout()
        plt.savefig(output_path)
        plt.close()
