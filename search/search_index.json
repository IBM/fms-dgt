{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to DGT","text":"<p>High-quality data is the backbone of modern AI development, but acquiring diverse, domain-specific, and scalable datasets remains a major bottleneck. Synthetic data generation addresses this challenge by enabling the creation of tailored datasets that are:</p> <ul> <li>Cost-effective and privacy-preserving</li> <li>Customizable for specific tasks and domains</li> <li>Scalable to meet evolving model needs</li> </ul> <p>DGT (Data Generation and Transformation) [pronounced \"digit\"] is a horizontal framework designed to streamline and scale expert, domain-specific synthetic data generation via simplifying and standardizing essential components.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83e\udd16 Standardize interface for ~5+ different LM engines (WatsonX, OpenAI, Azure OpenAI, vLLM, ollama, anthropic etc.) with retry/fallback logic</li> <li>\ud83d\udca1 Support for several domain-specific pipelines for tool calling, time series, question answering and more</li> <li>\ud83e\uddea Growing list of syntactic validators, deduplicators, LLMaJs (LLM-as-a-Judge)</li> <li>\ud83d\udd12 Local execution capabilities for sensitive data and air-gapped environments</li> <li>\ud83e\udd16 Plug-and-play [integrations][integrations] incl. Docling</li> <li>\ud83d\udcbb Simple and convenient CLI</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Before you start, please make sure you have Python 3.10+ available.</p> <p>Building DGT from source lets you make changes to the code base. To install from source, clone the repository and install with the following commands:</p> <pre><code>git clone git@github.com:IBM/fms-dgt.git\ncd fms-dgt\n</code></pre> <p>Now let's set up your virtual environment.</p> Python venvuv <pre><code>python3.10 -m venv ssdg_venv\nsource ssdg_venv/bin/activate\n</code></pre> <p>To install packages, we recommend starting off with the following</p> <pre><code>pip install -e \".[all]\"\n</code></pre> <p>If you plan on contributing, install the pre-commit hooks to keep code formatting clean</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <pre><code>uv sync --extra all\n</code></pre> <p>If you plan on contributing, install the pre-commit hooks to keep code formatting clean</p> <pre><code>uv pip install pre-commit\nuv pre-commit install\n</code></pre>"},{"location":"installation/#large-language-models-llms-dependencies","title":"Large Language Models (LLMs) Dependencies","text":"<p>DGT uses Large Language Models (LLMs) to generate synthetic data. Following LLM inference engines are supported:</p> Engine Additional Installation Environment Variables Supported APIs Ollama - - <code>completion</code>, <code>chat_completion</code> WatsonX - <code>WATSONX_API_KEY=\"\"</code>, <code>WATSONX_PROJECT_ID=\"\"</code> <code>completion</code>, <code>chat_completion</code> OpenAI - <code>OPENAI_API_KEY=\"\"</code> <code>completion</code>, <code>chat_completion</code> Azure OpenAI - <code>AZURE_OPENAI_API_KEY=\"\"</code> <code>completion</code>, <code>chat_completion</code> Anthropic Claude - <code>ANTHROPIC_API_KEY=\"\"</code> <code>chat_completion</code> vLLM <code>pip install -e \".[vllm]\"</code> - <code>completion</code>, <code>chat_completion</code> <p>Most of the aforementioned LLM inference engines use environment variables to specify configuration settings. You can either export those environment variables prior to every run or save them in <code>.env</code> file at base of <code>fms-dgt</code> repository directory.</p> <p>Warning</p> <p>vLLM dependencies requires Linux OS and CUDA.</p>"},{"location":"usage/","title":"Usage","text":"<p>Once you have successfully installed DiGiT, let's move on to creating your first synthetic data.</p> <p>In this example, we will be generating question answering (QA) pairs demonstrating logical reasoning. Try running the following command from the DiGiT source code directory</p> <pre><code>python -m fms_dgt.core --task-paths tasks/core/simple/logical_reasoning/causal/task.yaml --restart\n</code></pre> Info <p>This example uses the <code>SimpleDataBuilder</code> as defined here.</p> <pre><code>The `SimpleDataBuilder` relies on large language model (LLM) hosted via Ollama to generate data as specified [here](https://github.com/IBM/fms-dgt/blob/main/fms_dgt/core/databuilders/simple/simple.yaml).\n</code></pre> <p>You should see the following messages in your terminal</p> <pre><code>2025-10-28:23:20:11,301 INFO     [utils.py:109] Cannot find prompt.txt. Using default prompt depending on model-family.\n2025-10-28:23:20:11,302 INFO     [databuilder.py:540] ***************************************************************************************************\n2025-10-28:23:20:11,302 INFO     [databuilder.py:541]               EPOCH: 1\n2025-10-28:23:20:11,302 INFO     [databuilder.py:542] ***************************************************************************************************\nRunning completion requests: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:07&lt;00:00,  7.23s/it]\n2025-10-28:23:20:18,615 INFO     [generate.py:92] Request 1 took 7.31s, post-processing took 0.00s\n2025-10-28:23:20:18,629 INFO     [generate.py:117] Assessing generated samples took 0.01s, discarded 0 instances\n2025-10-28:23:20:18,630 INFO     [databuilder.py:581] ***************************************************************************************************\n2025-10-28:23:20:18,630 INFO     [databuilder.py:582]   [EPOCH 1]   GENERATION RESULTS AFTER ATTEMPT 1 (TOTAL ATTEMPTS: 1) # (1)!\n2025-10-28:23:20:18,630 INFO     [databuilder.py:588] ***************************************************************************************************\n2025-10-28:23:20:18,630 INFO     [databuilder.py:589] Task                                      Current         Total\n2025-10-28:23:20:18,630 INFO     [databuilder.py:595] core/simple/logical_reasoning/causal          1                1\n2025-10-28:23:20:18,630 INFO     [databuilder.py:597] ***************************************************************************************************\n2025-10-28:23:20:18,630 INFO     [default.py:189] No more rows left in dataloader. Resetting index to 0.\nRunning completion requests: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:07&lt;00:00,  7.26s/it] # (2)!\n2025-10-28:23:20:25,907 INFO     [generate.py:92] Request 2 took 7.27s, post-processing took 0.00s\n2025-10-28:23:20:25,915 INFO     [generate.py:117] Assessing generated samples took 0.01s, discarded 0 instances\n2025-10-28:23:20:25,915 INFO     [databuilder.py:581] ***************************************************************************************************\n2025-10-28:23:20:25,915 INFO     [databuilder.py:582]   [EPOCH 1]   GENERATION RESULTS AFTER ATTEMPT 2 (TOTAL ATTEMPTS: 2) # (3)!\n2025-10-28:23:20:25,915 INFO     [databuilder.py:588] ***************************************************************************************************\n2025-10-28:23:20:25,915 INFO     [databuilder.py:589] Task                                      Current         Total\n2025-10-28:23:20:25,915 INFO     [databuilder.py:595] core/simple/logical_reasoning/causal          1                2\n2025-10-28:23:20:25,915 INFO     [databuilder.py:597] ***************************************************************************************************\n2025-10-28:23:20:25,915 INFO     [databuilder.py:625] Launch postprocessing\n2025-10-28:23:20:25,926 INFO     [databuilder.py:371] ***************************************************************************************************\n2025-10-28:23:20:25,926 INFO     [databuilder.py:372]   [EPOCH 1]   POST-PROCESSING RESULTS\n2025-10-28:23:20:25,926 INFO     [databuilder.py:373] ***************************************************************************************************\n2025-10-28:23:20:25,927 INFO     [databuilder.py:374] Task                                      Before          After\n2025-10-28:23:20:25,927 INFO     [databuilder.py:380] core/simple/logical_reasoning/causal          2                2\n2025-10-28:23:20:25,927 INFO     [databuilder.py:382] ***************************************************************************************************\n2025-10-28:23:20:25,927 INFO     [databuilder.py:632] Postprocessing completed\n2025-10-28:23:20:25,927 INFO     [task.py:418] Saving final data to output/core/simple/logical_reasoning/causal/final_data.jsonl # (4)!\n2025-10-28:23:20:25,927 INFO     [databuilder.py:671] ***************************************************************************************************\n2025-10-28:23:20:25,927 INFO     [databuilder.py:674] Generation took 14.63s\n2025-10-28:23:20:25,928 INFO     [databuilder.py:288] ***************************************************************************************************\n2025-10-28:23:20:25,928 INFO     [databuilder.py:289]       EXECUTION PROFILER FOR DATABUILDER \"simple\" # (5)!\n2025-10-28:23:20:25,928 INFO     [databuilder.py:290] ***************************************************************************************************\n2025-10-28:23:20:25,928 INFO     [databuilder.py:291] Block                 Time (mean \u00b1 std)   Peak Memory (mean \u00b1 std)    Tokens (Completion Tokens) (Prompt)\n2025-10-28:23:20:25,928 INFO     [databuilder.py:270] generator             7.25 \u00b1 0.01         117.26 KB \u00b1 15.77 KB                152                 2,087\n2025-10-28:23:20:25,928 INFO     [databuilder.py:270] validator             0.0 \u00b1 0.0           256.0 B \u00b1 0B                         -                    -\n2025-10-28:23:20:25,928 INFO     [databuilder.py:270] rouge_val             0.0 \u00b1 0.0           2.47 KB \u00b1 0B                         -                    -\n2025-10-28:23:20:25,928 INFO     [databuilder.py:298] ***************************************************************************************************\n</code></pre> <ol> <li>Results summary after 1st generation attempt</li> <li>Start of the 2nd generation attempt</li> <li>Results summary after 2nd generation attempt</li> <li>Default location for generated synthetic data</li> <li>Execution summary</li> </ol> <p>Once generation is complete, let's examine the outputs which are saved in the <code>output/core/simple/logical_reasoning/causal/final_data.jsonl</code></p> <pre><code>{\"task_name\": \"core/simple/logical_reasoning/causal\", \"is_seed\": true, \"taxonomy_path\": \"core/simple/logical_reasoning/causal\", \"task_description\": \"To teach a language model about Logical Reasoning - causal relationships\", \"instruction\": \"Explain the causal relationship between smoking and lung cancer.\", \"input\": \"\", \"output\": \"Smoking is a major causal factor in the development of lung cancer. The chemicals in tobacco smoke can damage the DNA in lung cells, leading to uncontrolled cell growth and the formation of tumors.\", \"document\": null}\n{\"task_name\": \"core/simple/logical_reasoning/causal\", \"is_seed\": true, \"taxonomy_path\": \"core/simple/logical_reasoning/causal\", \"task_description\": \"To teach a language model about Logical Reasoning - causal relationships\", \"instruction\": \"Identify the causal relationship in the following scenario and explain it.\", \"input\": \"\\\"After increasing the number of police patrols in a neighborhood, the crime rate decreased significantly.\\\"\", \"output\": \"The causal relationship is that increasing the number of police patrols led to a decrease in the crime rate. The presence of more patrols likely acted as a deterrent to potential criminals, resulting in fewer crimes being committed.\", \"document\": null}\n</code></pre>"},{"location":"concepts/architecture/","title":"Architecture","text":"<p>DGT is a modular and extensible framework designed to support the generation of synthetic data across a wide range of use cases. It enables users to specify both what data they need and how it should be generated through two primary components: Tasks and Databuilders.</p>"},{"location":"concepts/architecture/#tasks","title":"Tasks","text":"<p>Tasks define the intent and requirements for synthetic data generation. They serve as high-level specifications that guide the overall data generation process.</p> <p>Each task includes:</p> <ul> <li>The type of data to be generated</li> <li>Required assets (e.g., schemas, seed data)</li> <li>Stopping criteria (e.g., number of records, time limits, quality thresholds)</li> </ul> <p>Tasks are declarative and reusable. While there is generally a many-to-one relationship between tasks and databuilders, a task can theoretically be fulfilled by multiple databuilders, provided the generated data adheres to the constraints defined by the databuilder.</p>"},{"location":"concepts/architecture/#databuilders","title":"Databuilders","text":"<p>Databuilders are the operational units responsible for implementing the logic to generate synthetic data. They transform task specifications into actual data outputs. They are typically opinonated and stateless in their design. Databuilders are engineered with broader applicability and maintainability in mind.</p>"},{"location":"concepts/architecture/#execution-model","title":"Execution Model","text":"<p>Parallelism is a core feature of DGT. It supports the execution of multiple tasks\u2014associated with the same or different databuilders\u2014within a single run.</p> <p>A typical run proceeds as follows:</p> <ul> <li>Initialization: All requested tasks are initialized, followed by their associated databuilders.</li> </ul> <ul> <li>Iteration Loop:<ul> <li>Each databuilder receives a set of incomplete tasks.</li> <li>It generates synthetic data points for those tasks.</li> </ul> </li> </ul> <ul> <li>Stopping Criteria Check: After each iteration, the framework checks whether stopping criteria (e.g., record count, stall limit, failure threshold) have been met for each task.</li> <li>Completion: The run ends when all tasks are either completed or exited.</li> </ul>"},{"location":"concepts/architecture/#additional-components","title":"Additional Components","text":""},{"location":"concepts/architecture/#blocks","title":"Blocks","text":"<p>Blocks are single-operation components that are initialized once per databuilder and shared across associated tasks. They promote logical isolation, reusability and minimal runtime memory footprint. Few prominent examples of blocks include:</p> <ul> <li>LMProvider Block: Connects to over six Language Model (LM) engines.</li> <li>Utility Blocks: Provide common operations such as deduplication, filtering, syntax checking, and LLM-as-a-Judge (LLMaJ) capabilities.</li> </ul>"},{"location":"concepts/architecture/#datastores-and-dataloaders","title":"Datastores and Dataloaders","text":"<p>Most data generation pipelines require loading external assets such as In-context learning (ICL) examples, knowledge documents, API specifications, prompt templates. DGT simplifies such asset management through Datastores and Dataloaders.</p>"},{"location":"concepts/blocks/","title":"Blocks","text":"<p>As mentioned previously, in the data builder <code>__call__</code> function, you will make use of blocks, which are the components in DGT that do the heavy lifting or contain specialized algorithms that would be useful to share across teams. While there are few constraints as to what types of programs can be written as blocks, for the most part, we have found that most blocks will fall under the general category of \"generators\" and \"validators\". To use a specific block in your databuilder, you need to specify it in both the YAML and in the <code>generate.py</code> file as a attribute of the databuilder's class. From a design standpoint, we aim to keep all multiprocessing and parallelism contained to the generators and the validators, i.e., not in the <code>__call__</code> function. By defining these ahead of time and restricting heavy operations to these objects, we can allow for better performance optimizations in terms of speed and resource allocation.</p> <p>To define a new block, first take a look at the base classes that the concrete implementation will inherit from. These are found in here. All blocks must define a <code>execute</code> function which contains their main logic.</p> <p>Blocks are designed to be composable and specifiable through both config and code. A block will take as its main inputs an iterable of dictionaries, a huggingface dataset, or a pandas dataframe (see <code>fms_dgt/base/block.py</code>). Internally, it operates over a dataclass that is linked in its class definition under the <code>DATA_TYPE</code> field (e.g., see LMBlockData). When the list of inputs (e.g., dictionaries) is passed to the block, the <code>__call__</code> function extracts the elements of the dictionary and maps them to the fields of the specified dataclass. This can be explicitly performed by specifying <code>input_map</code> or <code>output_map</code> in either the <code>__init__</code> of the block or in the call to the block. When neither <code>input_map</code> or <code>output_map</code> are specified, the DGT assumes the required fields (e.g., the fields of the internal block dataclass) will be present in the input / output objects.</p> <p>The core computation of the block (e.g., an LLM call in <code>fms_dgt/core/blocks/llm/llm.py</code>) is then run on those extracted dataclass objects and the results are written back to the original dictionaries (using the <code>output_map</code>).</p> <p>For example, within a databuilder one might call an llm block with:</p> <pre><code>inp = {\n  \"llm_input\": \"Respond 'yes' or 'no'\",\n  \"llm_params\" : {\"stop\": [\"yes\", \"no\"], \"temperature\": 1.0},\n}\ninp_lst = [inp]\nllm_outputs = llm_class(inp, input_map={\"llm_input\": \"input\", \"llm_params\": \"gen_kwargs\"}, output_map={\"result\": \"llm_result\"})\n</code></pre> <p>and the output may be extracted with</p> <pre><code>for llm_output in llm_outputs:\n  print(\"Original prompt: \" + llm_output[\"input\"])\n  print(f\"Result: {llm_output['llm_result']}\")\n</code></pre> <p>Importantly, the <code>output_map</code> will specify what is written onto the object that is passed in. Hence, if you want drag along additional elements in the dictionary, you just add those as fields to the input. Typically, in the process of SDG you are building up some object to return. This could be passed to through block as</p> <pre><code>inp = {\n  \"input\": \"Respond 'yes' or 'no'\",\n  \"gen_kwargs\": {\"stop\": [\"yes\", \"no\"], \"temperature\": 1.0},\n  \"data\": SdgObjectBeingGenerated\n}\ninp_lst = [inp]\n</code></pre>"},{"location":"concepts/databuilders/","title":"Data Builders","text":"<p>Tasks define what data to consume and produce, while Data Builders define how that data is produced. A Data Builder is a class responsible for implementing the logic that generates or transforms data. Each Data Builder exposes a call method, which operates on:</p> <ul> <li>Input: Accepts a list of dataclass instances.</li> <li>Output: Returns a list of dataclass instances.</li> </ul> <p>This design enables Data Builders to be reused across multiple tasks, as long as those tasks operate on the same type of dataclass. By decoupling data generation logic from task design, the framework promotes modularity and flexibility.</p> <p>There is built-in support for two prominent patterns data processing patterns viz. 1) Generation and 2) Transformation</p> Generation Transformation Purpose Create new synthetic data instances from scratch or seed examples Modify or convert existing data into a new form Input Often starts with empty list or minimal seed data Requires existing data instances Output Newly generated synthetic data Transformed version of input data Typical Use Case Data augmentation, synthetic dataset creation Translation, normalization, feature extraction Dependency May rely on generative models or iterative refinement Depends on transformation logic or mapping rules Processing Style Iterative: continues until target number of datapoints reached Single-pass: processes each input once Example Generate synthetic sentences from seed text Translate English sentences to French"},{"location":"concepts/databuilders/#effeciency-via-blocks","title":"Effeciency via Blocks","text":"<p>We designed the framework to be non-prescriptive, allowing flexible implementation of the <code>__call__</code> function. However, we strongly encourage the use of blocks for computationally intensive operations, such as batch processing with LLMs (predefined blocks for LLMs are available here).</p> <p>Adding a block to a data builder is straightforward. The framework automatically handles block initialization. To include a block, simply declare a class variable in the data builder class and configure it in the accompanying YAML file by adding an entry to the blocks list. For instance, the GeographyQA data builder uses a <code>generator</code> block:</p> fms_dgt/public/databuilders/examples/qa/generate.py<pre><code>@register_data_builder(\"public/examples/geography_qa\")\nclass GeographyQADataBuilder(GenerationDataBuilder):\n    \"\"\"Geography QA data builder\"\"\"\n\n    TASK_TYPE: GeographyQATask = GeographyQATask\n\n    # NOTE: generator is the language model that we will use to produce the synthetic examples\n    generator: LMProvider\n\n    def __init__(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        super().__init__(*args, **kwargs)\n\n    .\n</code></pre> <p>And the corresponding YAML configuration:</p> fms_dgt/public/databuilders/examples/qa/qa.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\nname: public/examples/geography_qa\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nblocks:\n  # Language model connector\n  - name: generator\n    type: ollama\n    model_id_or_path: mistral-small3.2\n    temperature: 0.0\n    max_tokens: 128\n  .\n</code></pre> Warning <p>Make sure the value specified for <code>model_id_or_path</code> field matches models available for the specified LLM backends.</p>"},{"location":"concepts/databuilders/#effeciency-via-parallelism","title":"Effeciency via Parallelism","text":"<p>Keep in mind that data builders can handle task parallelism, meaning multiple tasks can be processed simultaneously by the same data builder. This results in a mixed list of input data in the <code>__call__</code> function. When combining data from multiple tasks (e.g., for instruction following), ensure you track the origin of each data point.</p> <p>One can opt out of task parallelism by overriding the <code>call_with_task_list</code> method for <code>Generation</code> pattern, as shown below:</p> <pre><code>def call_with_task_list(self, tasks: List[Task], *args, **kwargs) -&gt; Iterable[DataPoint]:\n  for task in tasks:\n    # Tracks number of attempts made\n    request_idx = 0\n\n    data_pool = task.get_batch_examples()\n    while data_pool:\n      yield self(*[request_idx, data_pool], **dict())\n\n      # Fetch next batch of examples\n      data_pool = task.get_batch_examples()\n\n      # Update request index\n      request_idx += 1\n</code></pre>"},{"location":"concepts/dataloaders/","title":"Dataloaders","text":""},{"location":"concepts/datastores/","title":"Datastores","text":""},{"location":"concepts/tasks/","title":"Tasks","text":"<p>Tasks are simply classes which allows developers to define required assets (e.g., schemas, seed data, models), stopping criteria (e.g., number of records, time limits, quality thresholds), output data formatter as well as other high-level specifications that govern the overall data creation process.</p> <p>All Tasks in DGT must inherit from <code>Task</code> base class. Tasks are automatically instantiated using information provided in a configuration YAML file.</p> <p>The base <code>Task</code> class is intentionally minimal to avoid being overly prescriptive or burdensome for developers.</p>"},{"location":"concepts/tasks/#data-initialization","title":"Data Initialization","text":"<p>The Task class is responsible for initializing Datapoint objects, which are then passed to the data builder's <code>__call__</code> method.</p>"},{"location":"concepts/tasks/#input","title":"Input","text":"<p>By default, DGT uses the instantiate_input_example method to create input examples. This method receives kwargs constructed from either:</p> <ul> <li>A combination of seed_examples and synthetic_examples (for generation tasks), or</li> <li>Individual data instances (for transformation tasks)</li> </ul> <pre><code>def instantiate_input_example(self, **kwargs: Any) -&gt; INPUT_DATA_TYPE:\n        \"\"\"Instantiate an input example for this task. Designed to be overridden with custom initialization.\n\n        Args:\n            kwargs (Dict, optional): Kwargs used to instantiate an input example object.\n\n        Returns:\n            INPUT_DATA_TYPE: An instance of INPUT_DATA_TYPE.\n        \"\"\"\n        return self.INPUT_DATA_TYPE(task_name=kwargs.pop(\"task_name\", self.name), **kwargs)\n</code></pre> <p><code>INPUT_DATA_TYPE</code> is a dataclass that inherits from Datapoint and is defined as a class variable in task.py.</p>"},{"location":"concepts/tasks/#output","title":"Output","text":"<p>Similarly, output examples are created using the instantiate_output_example method. It receives kwargs built from the data returned by the data builder.</p> <pre><code>def instantiate_output_example(self, **kwargs: Any) -&gt; OUTPUT_DATA_TYPE:\n        \"\"\"Instantiate an output example for this task. Designed to be overridden with custom initialization.\n\n        Args:\n            kwargs (Dict, optional): Kwargs used to instantiate an output example object.\n\n        Returns:\n            OUTPUT_DATA_TYPE: An instance of OUTPUT_DATA_TYPE.\n        \"\"\"\n        return self.OUTPUT_DATA_TYPE(**kwargs)\n</code></pre> <p><code>OUTPUT_DATA_TYPE</code> is also a dataclass that inherits from Datapoint and is defined as a class variable in task.py.</p>"},{"location":"concepts/tasks/#stopping-criteria","title":"Stopping Criteria","text":"<p>DGT uses sensible default stopping criteria depending on the task type, but developers are encouraged to override them as needed.</p> <ul> <li> <p><code>Generation Tasks</code>: Completes once the minimum requested number of outputs is generated. Note that this number acts as a lower bound\u2014depending on the data builder's design, the final number of samples may exceed this value.</p> </li> <li> <p><code>Transformation Tasks</code>: Completes after a single pass over all data to be transformed.</p> </li> </ul> <p>To customize this behavior, developers can override the is_complete method.</p> <p>??? warning - <code>is_complete</code> function does not take any arguments</p>"},{"location":"concepts/tasks/#saving-data","title":"Saving Data","text":"<p>In DGT, the Task determines how and when data is saved. By default, DGT stores various types of data in a directory specified by the <code>DGT_OUTPUT_DIR</code> environment variable. If not set, it defaults to an output folder at the <code>root</code> of the repository. Data is saved using datastores, and includes the following</p> <ul> <li><code>task_card</code>: Contains all configuration details passed to the run.</li> <li><code>data</code>: Generated during the iterative loop execution. Each loop's output is appended and stored as intermediate data, saved before any post-processing occurs.</li> <li><code>final_data</code>: The validated and post-processed data. If no post-processor or validator is defined, this will be identical to <code>data</code>.</li> <li><code>postproc_data_*</code>: Data points after post-processing steps.</li> <li><code>formatted_data</code>: If a custom formatter is specified, this contains the final_data after formatting.</li> <li><code>task_results</code>: Logs task execution metadata such as start/end time, process ID, number of data points processed, and any custom metrics.</li> </ul> <p>You can override the default datastore behavior via the <code>YAML</code> configuration file. For example:</p> <pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\ntask_name: &lt;TASK_NAME&gt;\ntask_description: &lt;TASK_DESCRIPTION&gt;\ncreated_by: &lt;CREATOR&gt;\ndata_builder: &lt;DATABUILDER_NAME&gt; # Must match exactly with the databuilder name\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\ndatastore:\n    type: default\n    output_data_format: jsonl | parquet # (1)!\n</code></pre>"},{"location":"concepts/tasks/#fields","title":"Fields","text":"<p>DGT's runtime automatically initializes <code>Task</code> objects from task configuration YAML files. It does this by passing the contents of the YAML configuration as <code>kwargs</code> to the <code>Task</code> constructor.</p>"},{"location":"concepts/tasks/#mandatory-fields","title":"Mandatory Fields","text":"<ul> <li><code>task_name (str)</code>: A unique identifier for the task. Recommend following directory structure as naming convention.</li> <li><code>task_description (str)</code>: A detailed explanation of the use case.</li> <li><code>created_by (str)</code>: Information about the task creator.</li> <li><code>data_builder (str)</code>: Specifies the data builder to be used.</li> <li><code>seed_examples (List[dict] | None)</code>: In-context learning (ICL) examples used during the generative cycle. Applicable only when extending from GenerationTask.</li> <li><code>seed_datastore (Dict | None)</code>: Configuration for a datastore containing ICL examples used in the generative cycle. Applicable only when extending from GenerationTask.</li> <li><code>data (List[dict] | Dict)</code>: Input data to be transformed. Can be a list of dictionaries or a datastore configuration. Applicable only when extending from TransformationTask.</li> </ul>"},{"location":"concepts/tasks/#reserved-fields","title":"Reserved Fields","text":"<ul> <li><code>formatter (Dict | None)</code>: Defines the formatter and its configuration</li> <li><code>runner_config (Dict | TaskRunnerConfig)</code>: Specifies parameters that control execution behavior, such as <code>save_formatted_output</code>, <code>seed_batch_size</code>, and <code>transform_batch_size</code>. Many of these can be overridden via command-line arguments.</li> <li><code>datastore (Dict | None)</code>: Provides configuration for storing intermediate, final, and formatted data in a datastore.</li> </ul>"},{"location":"concepts/tasks/#extra-fields","title":"Extra Fields","text":"<p>Databuilder developers can easily pass custom fields to a task by modifying its constructor. As mentioned earlier, all entries from the YAML configuration file are provided to the constructor as <code>kwargs</code>.</p>"},{"location":"concepts/tasks/#example-passing-a-random-seed","title":"Example: Passing a Random Seed","text":"<p>Suppose we want to pass a <code>random_seed</code> value for selecting in-context learning (ICL) examples. First, add the field to your YAML configuration:</p> <pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\ntask_name: core/simple/logical_reasoning/causal # Must be unique. Recommend following directory structure as naming convention.\ntask_description: To teach a language model about Logical Reasoning - causal relationships\ncreated_by: IBM\ndata_builder: simple # Must match exactly with the databuilder name\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nseed_examples:\n  - answer:\n      \"While days tend to be longer in the summer, just because it is not summer\n      doesn't mean days are necessarily shorter.\n\n      \"\n    question:\n      \"If it is summer, then the days are longer. Are the days longer if it\n      is not summer ?\n\n      \"\n  - answer:\n      'No, we cannot conclusively conclude that some cats are black based solely\n      on the given premises. The statement \"some mammals are black\" does not necessarily\n      guarantee that among those mammals are cats.\n\n      '\n    question:\n      If all cats are mammals and some mammals are black, can we conclude that\n      some cats are black?\n  - answer:\n      \"Yes, we can conclude that all squares have four sides based on the given\n      premises.\n\n      \"\n    question:\n      \"If all squares are rectangles and a rectangle has four sides, can we\n      conclude that all squares have four sides?\n\n      \"\n\n######################################################\n#                   TASK FIELDS\n######################################################\nrandom_seed: 42\n</code></pre> <p>Next, update the associated <code>task.py</code> by adding a new argument to the <code>SimpleTask</code> constructor:</p> <pre><code># Standard\nfrom typing import Any, Dict, List, Optional\n\n# Local\nfrom fms_dgt.base.task import GenerationTask\nfrom fms_dgt.core.databuilders.simple.data_objects import SimpleData\n\n\nclass SimpleTask(GenerationTask):\n    \"\"\"This class is intended to hold general task information\"\"\"\n\n    INPUT_DATA_TYPE = SimpleData\n    OUTPUT_DATA_TYPE = SimpleData\n\n    def __init__(\n        self,\n        *args,\n        seed_datastore: Optional[Dict] = None,\n        seed_examples: Optional[List[Any]] = None,\n        random_seed: Optional[int] = None\n        **kwargs,\n    ):\n        # Step 1: Raise error if seed examples or seed datastore are not specified\n        if (seed_examples is None or not seed_examples) and seed_datastore is None:\n            raise ValueError(\n                \"Missing mandatory value for seed_examples or seed_datastore. Please provide at least one seed example in the task.yaml file before running.\"\n            )\n\n        # Step 2: Initialize parent\n        super().__init__(\n            *args, seed_datastore=seed_datastore, seed_examples=seed_examples, **kwargs\n        )\n\n        # Save 3: Save task sepcified fields for later use\n        self._random_seed = random_seed\n\n    def instantiate_input_example(self, **kwargs: Any):\n        return self.INPUT_DATA_TYPE(\n            task_name=self.name,\n            taxonomy_path=self.name,\n            task_description=self.task_description,\n            instruction=kwargs.get(\"question\", kwargs.get(\"instruction\")),\n            input=kwargs.get(\"context\", kwargs.get(\"input\", \"\")),\n            output=kwargs.get(\"answer\", kwargs.get(\"output\")),\n            document=kwargs.get(\"document\", None),\n        )\n</code></pre> <p>Now, the <code>_random_seed</code> attribute is available on the <code>SimpleTask</code> instance and can be used in any method within the task.</p>"},{"location":"examples/generate_data/","title":"Data Generation","text":"<p>In this example, we'll walk through the process of building a custom generation databuilder from scratch using DGT. You'll learn how to set up the necessary directory structure, define your task and data instance classes, and prepare your builder for generating data. For this example, we'll focus on a simple geography question-answering task.</p>"},{"location":"examples/generate_data/#set-up-your-directory","title":"Set Up Your Directory","text":"<p>We'll begin by creating the base directory that will house all the code for our SDG (Structured Data Generation) process. Run the following command from the root of your cloned repository:</p> <pre><code># from the root of the cloned repository\nmkdir -p fms_dgt/public/databuilders/test/geography_qa\n</code></pre> <p>This creates a new folder for your custom databuilder: <code>fms_dgt/public/databuilders/test/geography_qa</code></p>"},{"location":"examples/generate_data/#define-the-task-and-data-classes","title":"Define the Task and Data Classes","text":"<p>Next, we'll define the core objects that represent our task and data. In DGT, each databuilder typically includes:</p> <ul> <li>A Task class: representing the overall structure and metadata of the task.</li> <li>A DataPoint class: representing individual input and generated examples or records.</li> </ul> <p>Since we're working with a simple question-answering format, our classes will be minimal. Create a new file at: <code>fms_dgt/public/databuilders/test/geography_qa/task.py</code> and the following code to define your task and instance structure:</p> fms_dgt/public/databuilders/test/geography_qa/task.py<pre><code># Standard\nfrom dataclasses import dataclass\nfrom typing import Any\n\n# Local\nfrom fms_dgt.base.data_objects import DataPoint\nfrom fms_dgt.base.task import GenerationTask\n\n\n@dataclass(kw_only=True)\nclass GeographyQAData(DataPoint):\n    \"\"\"This class is intended to hold the seed / machine generated instruction data\"\"\"\n\n    question: str\n    answer: str\n\n\nclass GeographyQATask(GenerationTask):\n    \"\"\"\n    In this example, we wish to create 50 geographical question answer pairs. Hence, we choose to extend from `GenerationTask`.\n    \"\"\"\n\n    # We must always specify both the type of data that will be accepted as well as the type of data that will be generated\n    # For our example, we will be providing some seed examples to large language model to create new synthetic data in the similar format.\n    # Therefore, our `INPUT_DATA_TYPE` and `OUTPUT_DATA_TYPE` are identical.\n    #\n    # CAUTION: Be careful when you use different `INPUT_DATA_TYPE` and `OUTPUT_DATA_TYPE`. By default, `GenerationTask` type task are expected\n    # to keep looping over a mixture of seed and synthetic data till it produces requested number of synthetic data points.\n\n    INPUT_DATA_TYPE = GeographyQAData\n    OUTPUT_DATA_TYPE = GeographyQAData\n\n    def __init__(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Additional arguments specified in this constructor can be set using associated `task.yaml`\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n    def instantiate_input_example(self, **kwargs) -&gt; GeographyQAData:\n        \"\"\"\n        This helper method is called automatically on each seed data point provided to the generation task.\n\n        By default, it will try to instantiate object of `INPUT_DATA_TYPE` dataclass from each loaded data point. But, the\n        databuilder developer has ability to change the default behavior via overriding this method.\n\n        Returns:\n            GeographyQAData: object of `GeographyQAData` from seed data\n        \"\"\"\n        return GeographyQAData(\n            task_name=self.name,\n            is_seed=True,\n            question=kwargs.get(\"question\"),\n            answer=kwargs.get(\"answer\"),\n        )\n</code></pre> <p>This sets up the basic scaffolding for your databuilder. You can now proceed to implement the actual generation logic in a separate builder file.</p>"},{"location":"examples/generate_data/#implement-the-data-generation-logic","title":"Implement the Data Generation Logic","text":"<p>Now that we've defined our task and data classes, it's time to implement the actual data generation logic. This is where we use a language model to synthesize new question-answer pairs based on seed examples.</p> <p>Create a new file at: <code>fms_dgt/public/databuilders/test/geography_qa/generate.py</code> and add the following code to define your custom generation databuilder:</p> fms_dgt/public/databuilders/test/geography_qa/generate.py<pre><code># Standard\nfrom typing import Any, Dict, List\nimport random\n\n# Local\nfrom fms_dgt.base.databuilder import GenerationDataBuilder\nfrom fms_dgt.base.registry import register_data_builder\nfrom fms_dgt.core.blocks.llm import LMProvider\nfrom fms_dgt.public.databuilders.test.geography_qa.task import (\n    GeographyQAData,\n    GeographyQATask,\n)\n\n# NOTE: we register the data builder with the below decorator so that we can reference it in an input data file later on\n@register_data_builder(\"public/test/geography_qa\")\nclass GeographyQADataBuilder(GenerationDataBuilder):\n    \"\"\"Geography QA data builder\"\"\"\n\n    TASK_TYPE: GeographyQATask = GeographyQATask\n\n    # NOTE: generator is the language model that we will use to produce the synthetic examples\n    generator: LMProvider\n\n    def __init__(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        super().__init__(*args, **kwargs)\n\n        self._prompt_template = (\n            \"You are a geography question-answering data generator.\"\n            \" Your task is to come up with geography-related question-answer pairs that can be used to train a question-answering system.\"\n            \"\\n\\nHere are few examples:\\n\\n\"\n        )\n\n    def __call__(\n        self,\n        request_idx: int,\n        seed_data: List[GeographyQAData],\n    ) -&gt; List[GeographyQAData]:\n        # Build generator inputs\n        generator_inputs: List[Dict] = []\n        for _ in range(len(seed_data)):\n            # Randomly select in-context learning (icl) examples\n            icl_examples = random.choices(seed_data, k=3)\n\n            # Build prompt\n            prompt = f'{self._prompt_template}{\"\\n\\n\".join([f\"Question: {icl_example.question}\\nAnswer: {icl_example.answer}\" for icl_example in icl_examples])}\\n\\nNow generate a single different question-answer pair in the similar format.\\n\\n'\n\n\n            # Build generator inputs\n            # input (str | List[Dict[str, Any]]): (Reserved field) prompt to be passed to `/completion` endpoint or messages to be passed to `/chat/completion` endpoint\n            # gen_kwargs (Optional[Dict[str, Any]]): (Reserved field) Additional generation specific parameters to be passed to `/completion` or `/chat/completion` endpoint\n            # reference (Optional[Any]): We recommend passing data used to build prompt for future use. DiGiT returns all non-reserved field in output from a block.\n            generator_inputs.append(\n                {\n                    \"input\": prompt,\n                    \"reference\": icl_examples,\n                }\n            )\n\n        # Execute block\n        # LMProvider block is optimized to perform asynchronous invocation of `/completion` or `/chat/completion` endpoint to enable batch processing.\n        generator_outputs = self.generator(generator_inputs)\n\n        # Process outputs from block\n        outputs = []\n        for generator_output in generator_outputs:\n            # Extract icl examples passed to LMProvider block\n            icl_examples = generator_output[\"reference\"]\n\n            # LMProvider block return output from `/completion` or `/chat/completion` endpoint in \"result\" field.\n            question_answer_pair = generator_output[\"result\"].split(\"Answer:\")\n\n            # Minimal check to guarantee well formed response\n            if len(question_answer_pair) == 2:\n                # For well-formed response, build \"GeographyQAData\" objects\n                # As you can observed, having \"reference\" (icl examples) is handy to able to set correct \"task_name\"\n                outputs.append(\n                    GeographyQAData(\n                        task_name=icl_examples[0].task_name,\n                        is_seed=False,\n                        question=question_answer_pair[0]\n                        .split(\"Question:\")[-1]\n                        .strip()\n                        .rstrip(\"\\n\"),\n                        answer=question_answer_pair[1].strip().rstrip(\"\\n\"),\n                    )\n                )\n\n        # Return generated synthetic data points\n        return outputs\n</code></pre>"},{"location":"examples/generate_data/#define-the-builder-configuration","title":"Define the Builder Configuration","text":"<p>Next step is to define a configuration file that describes how the databuilder should operate. This file provides metadata, specifies the components involved in generation, and configures post-processing steps. Create a new file at: <code>fms_dgt/public/databuilders/test/geography_qa/geography_qa.yaml</code> and add the following contents:</p> fms_dgt/public/databuilders/test/geography_qa/geography_qa.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\nname: public/test/geography_qa\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nblocks:\n  # Language model connector\n  - name: generator\n    type: ollama\n    model_id_or_path: mistral-small3.2\n    temperature: 0.0\n    max_tokens: 128\n  # Built-in Rouge-L score based deduplicator\n  - name: dedup\n    type: rouge_scorer\n    filter: true\n    threshold: 1.0\n    input_map:\n      question: input\npostprocessors:\n  # Post-processors operate on all data points simultaneously\n  - name: dedup\nmetadata:\n  version: 1.0\n</code></pre> <p>This configuration sets up the generation databuilder using a language model (mistral-small3.2) and includes a deduplication block to filter out near-duplicate questions using Rouge-L scoring. The name field uniquely identifies your databuilder, and the metadata section can be extended as needed.</p>"},{"location":"examples/generate_data/#create-a-task-file","title":"Create a Task File","text":"<p>With the databuilder code and configuration in place, the final step is to define a task file that will drive the SDG process. Every SDG workflow begins with a task.yaml file that specifies the task name, description, seed examples, and the associated databuilder. Start by creating the task directory:</p> <pre><code># from repo root\nmkdir -p tasks/public/test/geography_qa\n</code></pre> <p>Inside this directory, create a file named task.yaml with the following contents:</p> tasks/public/test/geography_qa/task.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\ntask_name: public/test/geography_qa\ntask_description: A task for geography question-answering\ncreated_by: IBM\n\ndata_builder: public/test/geography_qa\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nseed_examples:\n  - question: What is the name of the tallest mountain in the world?\n    answer: Mount Everest\n  - question: What are the names of the five oceans of the world?\n    answer: Atlantic, Pacific, Indian, Arctic, and the Antarctic\n  - question: What is the largest desert in the world?\n    answer: The Antarctic Desert\n  - question: What is the longest river in Africa?\n    answer: The Nile River\n  - question: What is the smallest country in the world by land area?\n    answer: Vatican City\n  - question: What is the capital of Australia?\n    answer: Canberra\n  - question: What is the longest mountain range in South America?\n    answer: The Andes mountain range\n  - question: What are well known dense forests around the world?\n    answer: Amazon Rainforest, Congo Basin, La Mosquitia jungle are few examples of dense rainforests with thick, nearly impenetrable vegetation.\n  - question: Which country has the largest population in the world?\n    answer: China\n  - question: What American city is the Golden Gate Bridge located in?\n    answer: San Francisco\n  - question: What is the capital of Mexico?\n    answer: Mexico City\n  - question: What is the name of the largest ocean in the world?\n    answer: The Pacific Ocean\n  - question: What country has the most natural lakes?\n    answer: Canada\n  - question: What continent is Britain part of?\n    answer: Europe\n  - question: Which European country is closest to Africa?\n    answer: Spain\n  - question: In what country is the Taj Mahal located?\n    answer: India\n  - question: What do you call a chain of mountains?\n    answer: A range\n  - question: How many time zones does Russia have?\n    answer: 11\n  - question: What is the name of the only tropical rainforest in the United States?\n    answer: Puerto Rico\u2019s El Yunque National Forest\n  - question: What country formerly ruled Iceland?\n    answer: Denmark\n</code></pre> <p>These seed examples will be used by the generation databuilder to produce additional synthetic question-answer pairs in a similar format.</p>"},{"location":"examples/generate_data/#running-your-code","title":"Running your Code","text":"<p>From the root of your repository, execute the following command:</p> <pre><code>python -m fms_dgt.public --task-path ./tasks/public/test/geography_qa/task.yaml\n</code></pre> <p>Once the process completes, the generated data will be available at:</p> <p><code>output/public/test/geography_qa/final_data.jsonl</code></p>"},{"location":"examples/generate_qa_over_knowledge/","title":"Generate question-answer (QA) pairs over knowledge documents","text":"<p>The knowledge databuilder is used for generating synthetic data in the form of question-answer (QA) pairs by leveraging external knowledge. The data generated is then can be used for knowledge-tuning a Large Language Model. The databuilder consists of two stages - generation and verification. Each stage uses a particular template abd has its own set of principles and instructions that control the role of the teacher model (generator vs evaluator) that help guide the generation/evaluation process.</p> <p>Tip</p> <p>This databuilder is an implementation of the LAB method described in Large-Scale Alignment for ChatBots.</p>"},{"location":"examples/generate_qa_over_knowledge/#running-knowledge-sdg-out-of-the-box","title":"Running Knowledge-SDG out-of-the-box","text":""},{"location":"examples/generate_qa_over_knowledge/#create-a-new-task-yaml-file","title":"Create a new task yaml file","text":"<p>For this exercise, let's create a task file that helps generate data for teaching a model about photosynthesis using external document(s).</p> <p>As described in the Task section, let's create a <code>task.yaml</code> file in the following directory.</p> <pre><code>$ mkdir data/knowledge/photosynthesis\n</code></pre> <p>Within this directory, add a <code>qna.yaml</code> file with the following lines:</p> <pre><code># data/knowledge/photosynthesis/qna.yaml\ntask_name: knowledge_photosynthesis\ncreated_by: IBM Research\ntask_description: \"To teach a language model about photosynthesis\"\n</code></pre> <p>Since we're using the <code>knowledge_sdg</code> databuilder, let's add:</p> <pre><code>data_builder: knowledge_sdg\n</code></pre> <p>Before defining the <code>seed_examples</code> let's look at the task definition of Knowledge-SDG to see if we missed any other fields. The <code>KnowledgeSdgTask</code> class doesn't define any additional fields, so we can go ahead with adding the <code>seed_examples</code>.</p> <p>If we take a look at <code>KnowledgeSdgData</code> class, we get an idea of the fields that need to be present inside the <code>seed_examples</code>. We note that there are 5 fields:</p> <ul> <li><code>taxonomy_path</code>: This will be auto-populated during instantiation, so we can skip this</li> <li><code>task_description</code>: This will also be auto-populated</li> <li><code>domain</code>: This needs to be provided.</li> <li><code>question</code>: This needs to be provided.</li> <li><code>answer</code>: This also needs to be provided.</li> <li><code>document</code>: This is an optional field depending on the task.</li> </ul> <p>Let's add in the <code>domain</code> field:</p> <pre><code>domain: plants\n</code></pre> <p>Let's now add in some seed examples:</p> <pre><code>seed_examples:\n  - answer: The word respiration is commonly used to describe the process of breathing in oxygen and breathing out carbon dioxide.\n    question: What is respiration?\n  - answer: An ecosystem is a community of organisms and their physical environment interacting together.\n    question: What is an ecosystem?\n  - answer: Metabolism is the chemical reactions in the body's cells that change food into energy.\n    question: What is metabolism?\n</code></pre> <p>Next, we need to specify the knowledge document for our task. We can do this using the <code>include</code> directive.</p> <pre><code>include:\n  documents:\n    photosynthesis: documents/photosynthesis.md\n</code></pre> <p>Let's also create a directory <code>documents</code> and a file <code>photosynthesis.md</code> inside <code>data/knowledge/photosynthesis</code></p> <pre><code>mkdir data/knowledge/photosynthesis/documents\n</code></pre> <pre><code>What is photosynthesis?\n\nPhotosynthesis is arguably the most important biological process on earth. By liberating oxygen and consuming carbon dioxide, it has transformed the world into the hospitable environment we know today. Directly or indirectly, photosynthesis fills all of our food requirements and many of our needs for fiber and building materials. The energy stored in petroleum, natural gas and coal all came from the sun via photosynthesis, as does the energy in firewood, which is a major fuel in many parts of the world. This being the case, scientific research into photosynthesis is vitally important. If we can understand and control the intricacies of the photosynthetic process, we can learn how to increase crop yields of food, fiber, wood, and fuel, and how to better use our lands. The energy-harvesting secrets of plants can be adapted to man-made systems which provide new, efficient ways to collect and use solar energy. These same natural \"technologies\" can help point the way to the design of new, faster, and more compact computers, and even to new medical breakthroughs. Because photosynthesis helps control the makeup of our atmosphere, understanding photosynthesis is crucial to understanding how carbon dioxide and other \"greenhouse gases\" affect the global climate. In this document, we will briefly explore each of the areas mentioned above, and illustrate how photosynthesis research is critical to maintaining and improving our quality of life.\n\nPhotosynthesis and food. All of our biological energy needs are met by the plant kingdom, either directly or through herbivorous animals. Plants in turn obtain the energy to synthesize foodstuffs via photosynthesis. Although plants draw necessary materials from the soil and water and carbon dioxide from the air, the energy needs of the plant are filled by sunlight. Sunlight is pure energy. However, sunlight itself is not a very useful form of energy; it cannot be eaten, it cannot turn dynamos, and it cannot be stored. To be beneficial, the energy in sunlight must be converted to other forms. This is what photosynthesis is all about. It is the process by which plants change the energy in sunlight to kinds of energy that can be stored for later use. Plants carry out this process in photosynthetic reaction centers. These tiny units are found in leaves, and convert light energy to chemical energy, which is the form used by all living organisms. One of the major energy-harvesting processes in plants involves using the energy of sunlight to convert carbon dioxide from the air into sugars, starches, and other high-energy carbohydrates. Oxygen is released in the process. Later, when the plant needs food, it draws upon the energy stored in these carbohydrates. We do the same. When we eat a plate of spaghetti, our bodies oxidize or \"burn\" the starch by allowing it to combine with oxygen from the air. This produces carbon dioxide, which we exhale, and the energy we need to survive. Thus, if there is no photosynthesis, there is no food. Indeed, one widely accepted theory explaining the extinction of the dinosaurs suggests that a comet, meteor, or volcano ejected so much material into the atmosphere that the amount of sunlight reaching the earth was severely reduced. This in turn caused the death of many plants and the creatures that depended upon them for energy.\n\nPhotosynthesis and energy. One of the carbohydrates resulting from photosynthesis is cellulose, which makes up the bulk of dry wood and other plant material. When we burn wood, we convert the cellulose back to carbon dioxide and release the stored energy as heat. Burning fuel is basically the same oxidation process that occurs in our bodies; it liberates the energy of \"stored sunlight\" in a useful form, and returns carbon dioxide to the atmosphere. Energy from burning \"biomass\" is important in many parts of the world. In developing countries, firewood continues to be critical to survival. Ethanol (grain alcohol) produced from sugars and starches by fermentation is a major automobile fuel in Brazil, and is added to gasoline in some parts of the United States to help reduce emissions of harmful pollutants. Ethanol is also readily converted to ethylene, which serves as a feedstock to a large part of the petrochemical industry. It is possible to convert cellulose to sugar, and then into ethanol; various microorganisms carry out this process. It could be commercially important one day.\n\nOur major sources of energy, of course, are coal, oil and natural gas. These materials are all derived from ancient plants and animals, and the energy stored within them is chemical energy that originally came from sunlight through photosynthesis. Thus, most of the energy we use today was originally solar energy!\n\nPhotosynthesis, fiber, and materials. Wood, of course, is not only burned, but is an important material for building and many other purposes. Paper, for example, is nearly pure photosynthetically produced cellulose, as is cotton and many other natural fibers. Even wool production depends on photosynthetically-derived energy. In fact, all plant and animal products including many medicines and drugs require energy to produce, and that energy comes ultimately from sunlight via photosynthesis. Many of our other materials needs are filled by plastics and synthetic fibers which are produced from petroleum, and are thus also photosynthetic in origin. Even much of our metal refining depends ultimately on coal or other photosynthetic products. Indeed, it is difficult to name an economically important material or substance whose existence and usefulness is not in some way tied to photosynthesis.\n\nPhotosynthesis and the environment. Currently, there is a lot of discussion concerning the possible effects of carbon dioxide and other \"greenhouse gases\" on the environment. As mentioned above, photosynthesis converts carbon dioxide from the air to carbohydrates and other kinds of \"fixed\" carbon and releases oxygen to the atmosphere. When we burn firewood, ethanol, or coal, oil and other fossil fuels, oxygen is consumed, and carbon dioxide is released back to the atmosphere. Thus, carbon dioxide which was removed from the atmosphere over millions of years is being replaced very quickly through our consumption of these fuels. The increase in carbon dioxide and related gases is bound to affect our atmosphere. Will this change be large or small, and will it be harmful or beneficial? These questions are being actively studied by many scientists today. The answers will depend strongly on the effect of photosynthesis carried out by land and sea organisms. As photosynthesis consumes carbon dioxide and releases oxygen, it helps counteract the effect of combustion of fossil fuels. The burning of fossil fuels releases not only carbon dioxide, but also hydrocarbons, nitrogen oxides, and other trace materials that pollute the atmosphere and contribute to long-term health and environmental problems. These problems are a consequence of the fact that nature has chosen to implement photosynthesis through conversion of carbon dioxide to energy-rich materials such as carbohydrates. Can the principles of photosynthetic solar energy harvesting be used in some way to produce non-polluting fuels or energy sources? The answer, as we shall see, is yes.\n</code></pre> <p>Source: https://bioenergy.asu.edu/why-study-photosynthesis</p>"},{"location":"examples/generate_qa_over_knowledge/#generate-data","title":"Generate data","text":"<p>We can now run the generation job</p> <pre><code>$ fms_dgt --data-path ./data/knowledge/photosynthesis --num-prompt-instructions 3\n</code></pre> <p>NOTE</p> <p><code>--num-prompt-instructions</code> is used to specify the no. of in-context learning examples to use in prompts</p> <p>The output will be written to <code>./output/knowledge_photosynthesis/data.jsonl</code></p> <p>Here's a sample output:</p> <pre><code>{\"task_name\": \"knowledge_photosynthesis\", \"taxonomy_path\": \"knowledge_photosynthesis\", \"task_description\": \"To teach a language model about photosynthesis\", \"domain\": \"plants\", \"question\": \"What is the role of photosynthesis in food production?\", \"answer\": \"Photosynthesis plays an essential role in food production as it is the process by which plants, algae, and some bacteria convert light energy, carbon dioxide, and water into glucose and oxygen. This process supplies food for the plant and releases oxygen, which is essential for the survival of most organisms. Animals, including humans, rely on plants as their primary food source, either by consuming plants directly or by consuming herbivores that have eaten plants. This energy transfer from plants to animals forms the basis of food chains and webs in ecosystems. Photosynthesis is, therefore, critical for food production and the survival of most life forms on Earth.\\n\", \"document\": {\"content\": \"What is photosynthesis? Photosynthesis is arguably the most important biological process on earth. By liberating oxygen and consuming carbon dioxide, it has transformed the world into the hospitable environment we know today. Directly or indirectly, photosynthesis fills all of our food requirements and many of our needs for fiber and building materials. The energy stored in petroleum, natural gas and coal all came from the sun via photosynthesis, as does the energy in firewood, which is a major fuel in many parts of the world. This being the case, scientific research into photosynthesis is vitally important. If we can understand and control the intricacies of the photosynthetic process, we can learn how to increase crop yields of food, fiber, wood, and fuel, and how to better use our lands. The energy-harvesting secrets of plants can be adapted to man-made systems which provide new, efficient ways to collect and use solar energy. These same natural \\\"technologies\\\" can help point the way to the design of new, faster, and more compact computers, and even to new medical breakthroughs. Because photosynthesis helps control the makeup of our atmosphere, understanding photosynthesis is crucial to understanding how carbon dioxide and other \\\"greenhouse gases\\\" affect the global climate. In this document, we will briefly explore each of the areas mentioned above, and illustrate how photosynthesis research is critical to maintaining and improving our quality of life. Photosynthesis and food. All of our biological energy needs are met by the plant kingdom, either directly or through herbivorous animals. Plants in turn obtain the energy to synthesize foodstuffs via photosynthesis. Although plants draw necessary materials from the soil and water and carbon dioxide from the air, the energy needs of the plant are filled by sunlight. Sunlight is pure energy. However, sunlight itself is not a very useful form of energy; it cannot be eaten, it cannot turn dynamos, and it cannot be stored. To be beneficial, the energy in sunlight must be converted to other forms. This is what photosynthesis is all about. It is the process by which plants change the energy in sunlight to kinds of energy that can be stored for later use. Plants carry out this process in photosynthetic reaction centers. These tiny units are found in leaves, and convert light energy to chemical energy, which is the form used by all living organisms. One of the major energy-harvesting processes in plants involves using the energy of sunlight to convert carbon dioxide from the air into sugars, starches, and other high-energy carbohydrates. Oxygen is released in the process. Later, when the plant needs food, it draws upon the energy stored in these carbohydrates. We do the same. When we eat a plate of spaghetti, our bodies oxidize or \\\"burn\\\" the starch by allowing it to combine with oxygen from the air. This produces carbon dioxide, which we exhale, and the energy we need to survive. Thus, if there is no photosynthesis, there is no food. Indeed, one widely accepted theory explaining the extinction of the dinosaurs suggests that a comet, meteor, or volcano ejected so much material into the atmosphere that the amount of sunlight reaching the earth was severely reduced. This in turn caused the death of many plants and the creatures that depended upon them for energy. Photosynthesis and energy. One of the carbohydrates resulting from photosynthesis is cellulose, which makes up the bulk of dry wood and other plant material. When we burn wood, we convert the cellulose back to carbon dioxide and release the stored energy as heat. Burning fuel is basically the same oxidation process that occurs in\", \"path\": \"knowledge_photosynthesis\", \"document_len\": 600, \"domain\": \"plants\"}}\n{\"task_name\": \"knowledge_photosynthesis\", \"taxonomy_path\": \"knowledge_photosynthesis\", \"task_description\": \"To teach a language model about photosynthesis\", \"domain\": \"plants\", \"question\": \"What is cellulose and how is it related to photosynthesis?\", \"answer\": \"Cellulose is a carbohydrate that results from photosynthesis. It is the primary structural component of plant cell walls and is the most abundant organic polymer on Earth. During photosynthesis, plants convert carbon dioxide from the air into glucose, starches, and other high-energy carbohydrates, including cellulose. Cellulose is a complex polysaccharide made up of long chains of glucose molecules. It is indigestible to humans and many other animals, but it is an important energy source for ruminants, such as cows and sheep, which have specialized digestive systems that can break down cellulose. When cellulose is burned as fuel, it is converted back to carbon dioxide and water, releasing the stored energy as heat.\\n\", \"document\": {\"content\": \"What is photosynthesis? Photosynthesis is arguably the most important biological process on earth. By liberating oxygen and consuming carbon dioxide, it has transformed the world into the hospitable environment we know today. Directly or indirectly, photosynthesis fills all of our food requirements and many of our needs for fiber and building materials. The energy stored in petroleum, natural gas and coal all came from the sun via photosynthesis, as does the energy in firewood, which is a major fuel in many parts of the world. This being the case, scientific research into photosynthesis is vitally important. If we can understand and control the intricacies of the photosynthetic process, we can learn how to increase crop yields of food, fiber, wood, and fuel, and how to better use our lands. The energy-harvesting secrets of plants can be adapted to man-made systems which provide new, efficient ways to collect and use solar energy. These same natural \\\"technologies\\\" can help point the way to the design of new, faster, and more compact computers, and even to new medical breakthroughs. Because photosynthesis helps control the makeup of our atmosphere, understanding photosynthesis is crucial to understanding how carbon dioxide and other \\\"greenhouse gases\\\" affect the global climate. In this document, we will briefly explore each of the areas mentioned above, and illustrate how photosynthesis research is critical to maintaining and improving our quality of life. Photosynthesis and food. All of our biological energy needs are met by the plant kingdom, either directly or through herbivorous animals. Plants in turn obtain the energy to synthesize foodstuffs via photosynthesis. Although plants draw necessary materials from the soil and water and carbon dioxide from the air, the energy needs of the plant are filled by sunlight. Sunlight is pure energy. However, sunlight itself is not a very useful form of energy; it cannot be eaten, it cannot turn dynamos, and it cannot be stored. To be beneficial, the energy in sunlight must be converted to other forms. This is what photosynthesis is all about. It is the process by which plants change the energy in sunlight to kinds of energy that can be stored for later use. Plants carry out this process in photosynthetic reaction centers. These tiny units are found in leaves, and convert light energy to chemical energy, which is the form used by all living organisms. One of the major energy-harvesting processes in plants involves using the energy of sunlight to convert carbon dioxide from the air into sugars, starches, and other high-energy carbohydrates. Oxygen is released in the process. Later, when the plant needs food, it draws upon the energy stored in these carbohydrates. We do the same. When we eat a plate of spaghetti, our bodies oxidize or \\\"burn\\\" the starch by allowing it to combine with oxygen from the air. This produces carbon dioxide, which we exhale, and the energy we need to survive. Thus, if there is no photosynthesis, there is no food. Indeed, one widely accepted theory explaining the extinction of the dinosaurs suggests that a comet, meteor, or volcano ejected so much material into the atmosphere that the amount of sunlight reaching the earth was severely reduced. This in turn caused the death of many plants and the creatures that depended upon them for energy. Photosynthesis and energy. One of the carbohydrates resulting from photosynthesis is cellulose, which makes up the bulk of dry wood and other plant material. When we burn wood, we convert the cellulose back to carbon dioxide and release the stored energy as heat. Burning fuel is basically the same oxidation process that occurs in\", \"path\": \"knowledge_photosynthesis\", \"document_len\": 600, \"domain\": \"plants\"}}\n</code></pre> <p>As you can see, the document gets automatically chunked. To control the size of the document chunks, you can specify the following field in the task yaml:</p> <pre><code>chunk_size: 800 # no. of tokens per document chunk\n</code></pre>"},{"location":"examples/transform_data/","title":"Data Transformation","text":"<p>In this example, we'll walk through the process of designing a custom transformation databuilder using DGT. Specifically, we'll demonstrate adding custom chain-of-thought (CoT) to the Grade School Math (GSM) dataset, which is useful for tasks that benefit from intermediate reasoning steps.</p>"},{"location":"examples/transform_data/#what-are-transformation-data-builders","title":"What Are Transformation Data Builders?","text":"<p>While generation databuilders focuses on expanding a small set of examples into a larger dataset, transformation databuilders are designed to modify existing datasets. These modifications can include:</p> <ul> <li>Format conversions (e.g., from raw text to structured JSON)</li> <li>Quality improvements (e.g., filtering noisy samples)</li> <li>Anonymization (e.g., removing sensitive information)</li> <li>Task adaptation (e.g., converting slot-filling data into instruction-tuning format)</li> </ul> <p>DGT supports transformation tasks using the same infrastructure as generation tasks\u2014including LLM integration, data loading, and output handling. The key difference is that transformation builders typically process each input example once, applying a defined transformation, rather than iterating over again and again till stopping criteria is met.</p>"},{"location":"examples/transform_data/#set-up-your-transformation-databuilder","title":"Set Up Your Transformation Databuilder","text":"<p>To begin, we'll create the base directory for our transformation databuilder. From the root of your repository, run:</p> <pre><code># from root of your repository\nmkdir -p fms_dgt/public/databuilders/test/gsm_cot\n</code></pre> <p>This directory will contain the logic for transforming Grade School Math (GSM) dataset from OpenAI into a chain-of-thought format.</p>"},{"location":"examples/transform_data/#define-the-task-and-data-classes","title":"Define the Task and Data Classes","text":"<p>Next, we will define the data classes that represent the input and output formats. GSM examples typically consist of a question and an answer. For our transformation, we will retain these fields under new names, making minor modifications to the \"answer\" field to remove any existing reasoning steps. Additionally, we will introduce a new field called \"thought,\" which will contain custom-generated reasoning steps.</p> <p>Create a new file at: <code>fms_dgt/public/databuilders/test/gsm_cot/task.py</code></p> <p>Add the following code:</p> <pre><code># Standard\nfrom dataclasses import dataclass\n\n# Local\nfrom fms_dgt.base.data_objects import DataPoint\nfrom fms_dgt.base.task import TransformationTask\n\n\n@dataclass(kw_only=True)\nclass GsmData(DataPoint):\n    question: str\n    answer: str\n\n    def __post_init__(self):\n        self.question = self.question.strip()\n        # NOTE: GSM8k on huggingface already has the answer included with an explanation, we'll strip out the explanation and just keep the number\n        self.answer = self.answer.split(\"####\")[-1].strip()\n\n\n@dataclass(kw_only=True)\nclass GsmCotData(DataPoint):\n    input: str\n    output: str\n    thought: str\n\n\nclass GsmCotTask(TransformationTask):\n    # We must always specify both the type of data i.e. pre-transform (INPUT_DATA_TYPE) and post-transform (OUTPUT_DATA_TYPE)\n\n    INPUT_DATA_TYPE = GsmData\n    OUTPUT_DATA_TYPE = GsmCotData\n</code></pre>"},{"location":"examples/transform_data/#implement-the-data-transformation-logic","title":"Implement the Data Transformation Logic","text":"<p>Now that we've defined our task and data classes, it's time to implement the actual data transformation logic. This is where we use a language model to synthesize new chain-of-thought (COT) reasoning steps for each question answer pair in the original dataset.</p> <p>Create a new file at: <code>fms_dgt/public/databuilders/test/gsm_cot/generate.py</code> and add the following code to define your custom transformation databuilder:</p> fms_dgt/public/databuilders/test/gsm_cot/generate.py<pre><code># Standard\nfrom typing import Any, Iterable, List\n\n# Local\nfrom fms_dgt.base.databuilder import TransformationDataBuilder\nfrom fms_dgt.base.registry import register_data_builder\nfrom fms_dgt.core.blocks.llm import LMProvider\nfrom fms_dgt.public.databuilders.test.gsm_cot.task import (\n    GsmData,\n    GsmCotData,\n    GsmCotTask,\n)\nfrom fms_dgt.base.prompt import JinjaPromptTemplate\n\n\n@register_data_builder(\"public/test/gsm_cot\")\nclass GsmCotDataBuilder(TransformationDataBuilder):\n    \"\"\"Class for GSM chain-of-thought task\"\"\"\n\n    TASK_TYPE: GsmCotTask = GsmCotTask\n\n    lm: LMProvider\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        super().__init__(**kwargs)\n\n        # configure prompt\n        self._prompt_template = JinjaPromptTemplate(\n            template=(\n                \"You are an intelligent tutoring assistant that helps students with math homework.\"\n                \" Given a question and its answer, explain how to solve the question step-by-step to achieve the answer.\"\n                ' When you are explaining the answer to the student, please preface your explanation with \"Let\\'s think step-by-step.\"\\n\\n'\n                \"Question: {{ question }}\\nAnswer: {{ answer }}\\nExplanation: \"\n            )\n        )\n\n    def __call__(\n        self,\n        data_points: List[GsmData],\n    ) -&gt; Iterable[GsmCotData]:\n        # Build LM inputs\n        llm_inputs = [\n            {\n                \"input\": self._prompt_template.encode(\n                    render_dict={\"question\": data_point.question, \"answer\": data_point.answer}\n                ),\n                \"stop\": [\"Question:\"],\n                \"source\": data_point,\n            }\n            for data_point in data_points\n        ]\n\n        # Invoke LM\n        lm_outputs = self.lm(llm_inputs)\n\n        # Process LM outputs\n        for lm_output in lm_outputs:\n            source_data_point: GsmData = lm_output[\"source\"]\n            # NOTE: we don't do any validation of the generated 'thought', however, in general that would be a good idea\n            thought = lm_output[\"result\"].strip()\n            # NOTE: here we yield from the data builder so that the data is saved immediately in intermediate datastore\n            yield GsmCotData(\n                task_name=source_data_point.task_name,\n                is_seed=False,\n                input=source_data_point.question,\n                output=source_data_point.answer,\n                thought=thought,\n            )\n</code></pre>"},{"location":"examples/transform_data/#define-the-builder-configuration","title":"Define the Builder Configuration","text":"<p>Next step is to define a configuration file that describes how the databuilder should operate. This file provides metadata, specifies the components involved in transformation. Create a new file at: <code>fms_dgt/public/databuilders/test/gsm_cot/gsm_cot.yaml</code> and add the following contents:</p> fms_dgt/public/databuilders/test/gsm_cot/gsm_cot.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\nname: public/test/gsm_cot\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nblocks:\n  # Language model connector\n  - name: lm\n    type: ollama\n    model_id_or_path: mistral-small3.2\n    temperature: 0.0\n    max_tokens: 512\nmetadata:\n  version: 1.0\n</code></pre> <p>This configuration sets up the transformation databuilder using a language model (mistral-small3.2). The name field uniquely identifies your databuilder, and the metadata section can be extended as needed.</p>"},{"location":"examples/transform_data/#create-a-task-file","title":"Create a Task File","text":"<p>With the databuilder code and configuration in place, the final step is to define a task file that will drive the SDG process. Every SDG workflow begins with a task.yaml file that specifies the task name, description, data, and the associated databuilder. Start by creating the task directory:</p> <pre><code># from repo root\nmkdir -p tasks/public/test/gsm_cot\n</code></pre> <p>Inside this directory, create a file named task.yaml with the following contents:</p> tasks/public/test/gsm_cot/task.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\ntask_name: public/test/gsm_cot\ntask_description: A task for adding custom Chain-of-Thought (COT) to Grade School Math (GSM) dataset from OpenAI\ncreated_by: IBM\n\ndata_builder: public/test/gsm_cot\n######################################################\n#                   RESERVED FIELDS\n######################################################\ndata:\n  type: default\n  data_path: ${DGT_DATA_DIR}/public/test/gsm8k_cot/train.jsonl\n</code></pre> <p>Warning</p> <p>You will need to have local copy of Grade School Math (GSM) dataset from OpenAI in <code>data/public/test/gsm8k_cot</code> directory. You can download it from OpenAI's Github respository with following command.</p> <pre><code>wget https://raw.githubusercontent.com/openai/grade-school-math/refs/heads/master/grade_school_math/data/train.jsonl\n</code></pre>"},{"location":"examples/transform_data/#running-your-code","title":"Running your Code","text":"<p>From the root of your repository, execute the following command:</p> <pre><code>python -m fms_dgt.research --task-path ./tasks/public/test/gsm_cot/task.yaml\n</code></pre> <p>Once the process completes, the generated data will be available at:</p> <p><code>output/public/test/gsm_cot/final_data.jsonl</code></p>"},{"location":"tutorials/changing_lm_engine/","title":"Changing the Language Model (LM) Engine","text":"<p>DGT offers built-in support for over five different language model (LM) engines (WatsonX, OpenAI, Azure OpenAI, vLLM, Ollama, and Anthropic) through the <code>LMProvider</code> block. As described in this section, blocks are single-operation components that are initialized once per databuilder. This design makes it easy to switch between LM engines by simply updating the databuilder YAML configuration.</p> <p>Typically, the YAML file for a databuilder is located in the same directory as <code>generate.py</code>, which serves as the entry point for that databuilder. Let\u2019s revisit our earlier example of generating geography-based question-answer pairs.</p> <p>The corresponding YAML file can be found at: <code>fms_dgt/public/databuilders/examples/qa/qa.yaml</code></p> <p>Here\u2019s a closer look at its contents:</p> fms_dgt/public/databuilders/examples/qa/qa.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\nname: public/examples/geography_qa\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nblocks:\n  # Language model connector\n  - name: generator # (1)!\n    type: ollama # (2)!\n    model_id_or_path: mistral-small3.2 # (3)!\n    temperature: 0.0\n    max_tokens: 128\n  # Built-in Rouge-L score based deduplicator\n  - name: dedup\n    type: rouge_scorer\n    filter: true\n    threshold: 1.0\n    input_map:\n      question: input\npostprocessors:\n  # Post-processors operate on all data points simultaneously\n  - name: dedup\nmetadata:\n  version: 1.0\n</code></pre> <ol> <li>Identifier for the LM block.</li> <li>Specifies the LM engine. Supported values include watsonx, openai, azure-openai, anthropic, vllm, vllm-remote, and ollama.</li> <li>The model identifier or path, which varies depending on the selected LM engine. Refer to the documentation for the specific engine to determine the correct value.</li> </ol> <p>Let's try via changing the model used from <code>mistral-small3.2</code> to <code>gemma3:1b</code> as follows</p> fms_dgt/public/databuilders/examples/qa/qa.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\nname: public/examples/geography_qa\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nblocks:\n  # Language model connector\n  - name: generator\n    type: ollama\n    model_id_or_path: gemma3:1b\n    temperature: 0.0\n    max_tokens: 128\n  # Built-in Rouge-L score based deduplicator\n  - name: dedup\n    type: rouge_scorer\n    filter: true\n    threshold: 1.0\n    input_map:\n      question: input\npostprocessors:\n  # Post-processors operate on all data points simultaneously\n  - name: dedup\nmetadata:\n  version: 1.0\n</code></pre>"},{"location":"tutorials/creating_validator/","title":"Creating a validator","text":""},{"location":"tutorials/creating_validator/#recap-of-blocks","title":"Recap of Blocks","text":"<p>Blocks are one of the way to contribute specialized algorithms or tools to DGT, making them accessible for other individuals to use. Each block takes as input a list of dictionary-like objects (e.g., a pandas table, a list of dictionaries, etc.).</p> <p>Additionally, blocks can accept input_map and output_map as arguments (see here), or these can be set during block initialization (see here).</p> <p>Internally, a block is expected to iterate over each input element and extract instances of its associated <code>DATA_TYPE</code>. The block\u2019s output is then written back to the input elements (typically dictionaries), as specified by the output_map.</p>"},{"location":"tutorials/creating_validator/#creating-a-new-block","title":"Creating a New Block","text":"<p>In this example, we\u2019ll define a Validator Block. Validator blocks are used to verify whether an input element, often a newly generated data point from SDG, is valid and should be returned to the user.</p> <p>Let\u2019s revisit the Data Generation example, where the goal was to build a geography question-answering pipeline. We\u2019ll continue with that context, but modify the objective: we now want to restrict the generated questions to factoid-type questions only.</p> <p>To enforce this, we\u2019ll apply a length constraint on the answers. If an answer exceeds a certain length, it will be flagged as invalid.</p> <p>To implement this, create a file at:</p> <p><code>fms_dgt/public/databuilders/test/geography_qa/blocks/length_constraint/block.py</code></p> <p>and add the following code:</p> fms_dgt/public/databuilders/test/geography_qa/blocks/length_constraint/block.py<pre><code># Standard\nfrom dataclasses import dataclass\nfrom typing import Tuple, Dict\n\n# Local\nfrom fms_dgt.base.registry import register_block\nfrom fms_dgt.base.block import ValidatorBlock\nfrom fms_dgt.base.data_objects import ValidatorBlockData\n\n\n@dataclass(kw_only=True)\nclass LengthValidatorData(ValidatorBlockData):\n    input: str\n\n\n@register_block(\"public/test/geography_qa/length_constraint\")\nclass LengthValidator(ValidatorBlock):\n    \"\"\"Class for length-constraint validator\"\"\"\n\n    # We must associate LengthValidatorData as this class's DATA_TYPE for the input dictionaries to be mapped to instances of LengthValidatorData\n    DATA_TYPE = LengthValidatorData\n\n    def __init__(\n        self,\n        *args,\n        max_num_words: int = 5,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            *args,\n            **kwargs,\n        )\n\n        if max_num_words is None or max_num_words &lt; 0:\n            raise ValueError(\"Expected 'max_num_words' parameter to be a non-negative number\")\n\n        self._max_num_words = max_num_words\n\n    def _validate(self, instance: LengthValidatorData) -&gt; Tuple[bool, Dict | None]:\n        # Calculate number of words in the input\n        num_words = len(instance.input.split())\n\n        # Perform validity check\n        is_valid = num_words &lt;= self._max_num_words\n\n        # Return\n        return is_valid, (\n            {\n                \"reason\": f\"Number of words in input ({num_words}) exceeds limit ({self._max_num_words}).\"\n            }\n            if not is_valid\n            else None\n        )\n</code></pre>"},{"location":"tutorials/creating_validator/#integrating-the-new-block-into-the-data-builder","title":"Integrating the New Block into the Data Builder","text":"<p>Next, we need to update the data builder and its configuration to incorporate the newly created block.</p> <p>Open the file: <code>fms_dgt/public/databuilders/test/geography_qa/generate.py</code></p> <p>and update it with the following code:</p> fms_dgt/public/databuilders/test/geography_qa/generate.py<pre><code># Standard\nfrom typing import Any, Dict, List\nimport random\n\n# Local\nfrom fms_dgt.base.databuilder import GenerationDataBuilder\nfrom fms_dgt.base.registry import register_data_builder\nfrom fms_dgt.core.blocks.llm import LMProvider\nfrom fms_dgt.public.databuilders.test.geography_qa.task import (\n    GeographyQAData,\n    GeographyQATask,\n)\nfrom fms_dgt.public.databuilders.test.geography_qa.blocks.length_constraint.block import (\n    LengthValidator,\n)\n\n\n# NOTE: we register the data builder with the below decorator so that we can reference it in an input data file later on\n@register_data_builder(\"public/test/geography_qa\")\nclass GeographyQADataBuilder(GenerationDataBuilder):\n    \"\"\"Geography QA data builder\"\"\"\n\n    TASK_TYPE: GeographyQATask = GeographyQATask\n\n    # Generator is the language model that we will use to produce the synthetic examples\n    generator: LMProvider\n\n    # Validator is the validator we defined in our `blocks` directory\n    validator: LengthValidator\n\n    def __init__(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        super().__init__(*args, **kwargs)\n\n        self._prompt_template = (\n            \"You are a geography question-answering data generator.\"\n            \" Your task is to come up with geography-related question-answer pairs that can be used to train a question-answering system.\"\n            \"\\n\\nHere are few examples:\\n\\n\"\n        )\n\n    def __call__(\n        self,\n        request_idx: int,\n        seed_data: List[GeographyQAData],\n    ) -&gt; List[GeographyQAData]:\n        # Build generator inputs\n        generator_inputs: List[Dict] = []\n        for _ in range(len(seed_data)):\n            # Randomly select in-context learning (icl) examples\n            icl_examples = random.choices(seed_data, k=3)\n\n            # Build prompt\n            encoded_icl_examples = \"\\n\\n\".join(\n                [\n                    f\"Question: {icl_example.question}\\nAnswer: {icl_example.answer}\"\n                    for icl_example in icl_examples\n                ]\n            )\n            prompt = f\"{self._prompt_template}{encoded_icl_examples}\\n\\nNow generate a single different question-answer pair in the similar format.\\n\\n\"\n\n            # Build generator inputs\n            # input (str | List[Dict[str, Any]]): (Reserved field) prompt to be passed to `/completion` endpoint or messages to be passed to `/chat/completion` endpoint\n            # gen_kwargs (Optional[Dict[str, Any]]): (Reserved field) Additional generation specific parameters to be passed to `/completion` or `/chat/completion` endpoint\n            # reference (Optional[Any]): We recommend passing data used to build prompt for future use. DiGiT returns all non-reserved field in output from a block.\n            generator_inputs.append(\n                {\n                    \"input\": prompt,\n                    \"reference\": icl_examples,\n                }\n            )\n\n        # Execute block\n        # LMProvider block is optimized to perform asynchronous invocation of `/completion` or `/chat/completion` endpoint to enable batch processing.\n        generator_outputs = self.generator(generator_inputs)\n\n        # Process outputs from block\n        generated_outputs = []\n        for generator_output in generator_outputs:\n            # Extract icl examples passed to LMProvider block\n            icl_examples = generator_output[\"reference\"]\n\n            # LMProvider block return output from `/completion` or `/chat/completion` endpoint in \"result\" field.\n            question_answer_pair = generator_output[\"result\"].split(\"Answer:\")\n\n            # Minimal check to guarantee well formed response\n            if len(question_answer_pair) == 2:\n                # For well-formed response, build \"GeographyQAData\" objects\n                # As you can observed, having \"reference\" (icl examples) is handy to able to set correct \"task_name\"\n                generated_outputs.append(\n                    GeographyQAData(\n                        task_name=icl_examples[0].task_name,\n                        is_seed=False,\n                        question=question_answer_pair[0]\n                        .split(\"Question:\")[-1]\n                        .strip()\n                        .rstrip(\"\\n\"),\n                        answer=question_answer_pair[1].strip().rstrip(\"\\n\"),\n                    )\n                )\n\n        # Arguments that are not in our block's DATA_TYPE class are ignored, so we wrap out GeographySdgData objects in a dictionary with a 'reference' key\n        # \"store_names\" is an optional parameter designed to store the results of filtered data.\n        validated_outputs = self.validator(\n            [\n                {\n                    \"input\": generated_output.answer,\n                    \"reference\": generated_output,\n                    \"store_names\": self.get_block_store_names(\n                        block_name=self.validator.name, task_name=generated_output.task_name\n                    ),\n                }\n                for generated_output in generated_outputs\n            ]\n        )\n\n        # Return validated synthetic data points\n        return [validated_output[\"reference\"] for validated_output in validated_outputs]\n</code></pre> <p>This update ensures that the length_constraint block is executed as part of the data generation pipeline. Specifically, it will validate each generated answer and filter out those that exceed the allowed length, helping enforce the factoid-only constraint.</p> <p>Your code now makes use of your new validator block, however, you must also make it visible in the data builder config. Open up <code>fms_dgt/research/databuilders/geography_qa/geography_qa.yaml</code> and update the config to be the following</p> fms_dgt/research/databuilders/geography_qa/geography_qa.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\nname: public/test/geography_qa\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nblocks:\n  # Language model connector\n  - name: generator\n    type: ollama\n    model_id_or_path: mistral-small3.2\n    temperature: 0.0\n    max_tokens: 128\n  # Factoid answer validator (using length as proxy)\n  - name: validator\n    type: public/test/geography_qa/length_constraint\n    max_num_words: 3\n    filter: true\n  # Built-in Rouge-L score based deduplicator\n  - name: dedup\n    type: rouge_scorer\n    filter: true\n    threshold: 1.0\n    input_map:\n      question: input\npostprocessors:\n  # Post-processors operate on all data points simultaneously\n  - name: dedup\nmetadata:\n  version: 1.0\n</code></pre>"},{"location":"tutorials/creating_validator/#running-your-code","title":"Running your Code","text":"<p>From the root of your repository, execute the following command:</p> <pre><code>python -m fms_dgt.public --task-path ./tasks/public/test/geography_qa/task.yaml --restart-generation\n</code></pre> <p>Once the process completes, the generated data will be available at:</p> <p><code>output/public/test/geography_qa/final_data.jsonl</code></p>"},{"location":"tutorials/loading_seed_examples_from_file/","title":"Loading seed examples from a file","text":"<p>In synthetic data generation, a common pattern involves using seed examples (also known as in-context learning (ICL) examples) in the prompt provided to the teacher model. To support this, DGT offers a base class called <code>GenerationTask</code>, which allows databuilder developers to specify seed examples either directly in the task YAML file or through external files in formats such as .jsonl, .json, or .parquet.</p> <p>Continuing with our earlier example of generating geography-related question-answer pairs, you can find the seed examples defined in the task YAML file.</p> <p>Let\u2019s take a closer look at its contents:</p> tasks/public/examples/qa/task.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\ntask_name: public/examples/geography_qa\ntask_description: A task for geography question-answering\ncreated_by: IBM\n\ndata_builder: public/examples/geography_qa\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nseed_examples:\n  - question: What is the name of the tallest mountain in the world?\n    answer: Mount Everest\n  - question: What are the names of the five oceans of the world?\n    answer: Atlantic, Pacific, Indian, Arctic, and the Antarctic\n  - question: What is the largest desert in the world?\n    answer: The Antarctic Desert\n  - question: What is the longest river in Africa?\n    answer: The Nile River\n  - question: What is the smallest country in the world by land area?\n    answer: Vatican City\n  - question: What is the capital of Australia?\n    answer: Canberra\n  - question: What is the longest mountain range in South America?\n    answer: The Andes mountain range\n  - question: What are well known dense forests around the world?\n    answer: Amazon Rainforest, Congo Basin, La Mosquitia jungle are few examples of dense rainforests with thick, nearly impenetrable vegetation.\n  - question: Which country has the largest population in the world?\n    answer: China\n  - question: What American city is the Golden Gate Bridge located in?\n    answer: San Francisco\n  - question: What is the capital of Mexico?\n    answer: Mexico City\n  - question: What is the name of the largest ocean in the world?\n    answer: The Pacific Ocean\n  - question: What country has the most natural lakes?\n    answer: Canada\n  - question: What continent is Britain part of?\n    answer: Europe\n  - question: Which European country is closest to Africa?\n    answer: Spain\n  - question: In what country is the Taj Mahal located?\n    answer: India\n  - question: What do you call a chain of mountains?\n    answer: A range\n  - question: How many time zones does Russia have?\n    answer: 11\n  - question: What is the name of the only tropical rainforest in the United States?\n    answer: Puerto Rico\u2019s El Yunque National Forest\n  - question: What country formerly ruled Iceland?\n    answer: Denmark\n</code></pre> <p>Instead of defining seed examples directly in the task YAML file, let\u2019s specify them in a separate .jsonl file. This approach improves modularity, making the seed examples easier to manage and reuse across different tasks. Save the following file as seed_examples.jsonl in the data/public/examples/qa directory.</p> seed_examples.jsonl<pre><code>{\"question\": \"What is the name of the tallest mountain in the world?\", \"answer\": \"Mount Everest\"}\n{\"question\": \"What are the names of the five oceans of the world?\", \"answer\": \"Atlantic, Pacific, Indian, Arctic, and the Antarctic\"}\n{\"question\": \"What is the largest desert in the world?\", \"answer\": \"The Antarctic Desert\"}\n{\"question\": \"What is the longest river in Africa?\", \"answer\": \"The Nile River\"}\n{\"question\": \"What is the smallest country in the world by land area?\", \"answer\": \"Vatican City\"}\n{\"question\": \"What is the capital of Australia?\", \"answer\": \"Canberra\"}\n{\"question\": \"What is the longest mountain range in South America?\", \"answer\": \"The Andes mountain range\"}\n{\"question\": \"What are well known dense forests around the world?\", \"answer\": \"Amazon Rainforest, Congo Basin, La Mosquitia jungle are few examples of dense rainforests with thick, nearly impenetrable vegetation.\"}\n{\"question\": \"Which country has the largest population in the world?\", \"answer\": \"China\"}\n{\"question\": \"What American city is the Golden Gate Bridge located in?\", \"answer\": \"San Francisco\"}\n{\"question\": \"What is the capital of Mexico?\", \"answer\": \"Mexico City\"}\n{\"question\": \"What is the name of the largest ocean in the world?\", \"answer\": \"The Pacific Ocean\"}\n{\"question\": \"What country has the most natural lakes?\", \"answer\": \"Canada\"}\n{\"question\": \"What continent is Britain part of?\", \"answer\": \"Europe\"}\n{\"question\": \"Which European country is closest to Africa?\", \"answer\": \"Spain\"}\n{\"question\": \"In what country is the Taj Mahal located?\", \"answer\": \"India\"}\n{\"question\": \"What do you call a chain of mountains?\", \"answer\": \"A range\"}\n{\"question\": \"How many time zones does Russia have?\", \"answer\": \"11\"}\n{\"question\": \"What is the name of the only tropical rainforest in the United States?\", \"answer\": \"Puerto Rico\u2019s El Yunque National Forest\"}\n{\"question\": \"What country formerly ruled Iceland?\", \"answer\": \"Denmark\"}\n</code></pre> <p>Now, we can reference the newly created seed_examples.jsonl file in our task YAML as shown below:</p> tasks/public/examples/qa/task.yaml<pre><code>######################################################\n#                   MANDATORY FIELDS\n######################################################\ntask_name: public/examples/geography_qa\ntask_description: A task for geography question-answering\ncreated_by: IBM\n\ndata_builder: public/examples/geography_qa\n\n######################################################\n#                   RESERVED FIELDS\n######################################################\nseed_datastore:\n    type: default\n    data_path: ${DGT_DATA_DIR}/public/examples/qa/seed_examples.jsonl\n</code></pre>"}]}